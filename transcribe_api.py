#!/usr/bin/env python3
"""
è¶…ã‚·ãƒ³ãƒ—ãƒ«æ–‡å­—èµ·ã“ã—ï¼†è¦ç´„ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆPhase 7 Stage 7-1ï¼‰
ä½¿ã„æ–¹: python transcribe_api.py <éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹>
æ©Ÿèƒ½: Gemini Audio APIæ–‡å­—èµ·ã“ã—ï¼ˆè©±è€…è­˜åˆ¥ä»˜ãï¼‰ + Gemini APIè¦ç´„
"""

import os
import sys
import subprocess
from pathlib import Path
from dotenv import load_dotenv
import google.generativeai as genai

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# Gemini API inline file size limit (20MB)
MAX_FILE_SIZE = 20 * 1024 * 1024  # 20MB in bytes


def split_audio_file(file_path, chunk_duration=600):
    """
    Split large audio file into chunks using ffmpeg

    å¼•æ•°:
        file_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        chunk_duration: ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã®ç§’æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10åˆ† = 600ç§’ï¼‰

    æˆ»ã‚Šå€¤:
        ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
    """
    file_path = Path(file_path)
    output_dir = file_path.parent / f"{file_path.stem}_chunks"
    output_dir.mkdir(exist_ok=True)

    # ffmpegã§åˆ†å‰²
    output_pattern = str(output_dir / f"chunk_%03d{file_path.suffix}")

    cmd = [
        'ffmpeg',
        '-i', str(file_path),
        '-f', 'segment',
        '-segment_time', str(chunk_duration),
        '-c', 'copy',
        output_pattern
    ]

    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode != 0:
        raise Exception(f"ffmpeg failed: {result.stderr}")

    # ä½œæˆã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
    chunks = sorted(output_dir.glob(f"chunk_*{file_path.suffix}"))
    return chunks


def transcribe_audio(file_path):
    """
    Gemini Audio APIã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—ï¼ˆè©±è€…è­˜åˆ¥ä»˜ãã€å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰

    å¼•æ•°:
        file_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆ.m4a, .mp3, .wavç­‰ï¼‰

    æˆ»ã‚Šå€¤:
        ã‚¿ãƒ—ãƒ«: (æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆ, è©±è€…æƒ…å ±ãƒªã‚¹ãƒˆ)
    """
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    model = genai.GenerativeModel("gemini-2.5-flash")

    file_size = os.path.getsize(file_path)
    file_path_obj = Path(file_path)
    mime_type = f"audio/{file_path_obj.suffix[1:]}" if file_path_obj.suffix else "audio/mpeg"

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆ20MBè¶…éã®å ´åˆã¯åˆ†å‰²ï¼‰
    if file_size > MAX_FILE_SIZE:
        print(f"  File size: {file_size / 1024 / 1024:.1f}MB (exceeds 20MB limit)")
        print(f"  Splitting into chunks...")

        chunks = split_audio_file(file_path)
        print(f"  Created {len(chunks)} chunks")

        # å„ãƒãƒ£ãƒ³ã‚¯ã‚’æ–‡å­—èµ·ã“ã—
        transcriptions = []
        all_speakers = {}

        for i, chunk_path in enumerate(chunks, 1):
            print(f"  Transcribing chunk {i}/{len(chunks)}...", end='\r')

            with open(chunk_path, "rb") as audio_file:
                audio_bytes = audio_file.read()

            prompt = """ã“ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—ã—ã¦ãã ã•ã„ã€‚
è©±è€…ã‚’è­˜åˆ¥ã—ã€ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

[Speaker 1] ç™ºè¨€å†…å®¹
[Speaker 2] ç™ºè¨€å†…å®¹

æ—¥æœ¬èªã§æ–‡å­—èµ·ã“ã—ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚"""

            response = model.generate_content(
                [prompt, {"mime_type": mime_type, "data": audio_bytes}]
            )

            transcriptions.append(response.text)

            # è©±è€…ã‚«ã‚¦ãƒ³ãƒˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
            text = response.text
            for line in text.split('\n'):
                if '[Speaker' in line:
                    speaker = line.split(']')[0].replace('[', '').strip()
                    if speaker not in all_speakers:
                        all_speakers[speaker] = 0
                    all_speakers[speaker] += 1

        print()  # æ”¹è¡Œ

        # ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤
        for chunk in chunks:
            chunk.unlink()
        chunks[0].parent.rmdir()

        # æ–‡å­—èµ·ã“ã—çµæœã‚’çµåˆ
        full_text = "\n\n".join(transcriptions)
        speakers = list(all_speakers.keys())

        return full_text, speakers

    else:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒ20MBä»¥ä¸‹ã®å ´åˆã¯é€šå¸¸å‡¦ç†
        with open(file_path, "rb") as audio_file:
            audio_bytes = audio_file.read()

        prompt = """ã“ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—ã—ã¦ãã ã•ã„ã€‚
è©±è€…ã‚’è­˜åˆ¥ã—ã€ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

[Speaker 1] ç™ºè¨€å†…å®¹
[Speaker 2] ç™ºè¨€å†…å®¹

æ—¥æœ¬èªã§æ–‡å­—èµ·ã“ã—ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚"""

        response = model.generate_content(
            [prompt, {"mime_type": mime_type, "data": audio_bytes}]
        )

        # è©±è€…æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        speakers = []
        for line in response.text.split('\n'):
            if '[Speaker' in line:
                speaker = line.split(']')[0].replace('[', '').strip()
                if speaker not in speakers:
                    speakers.append(speaker)

        return response.text, speakers

def summarize_text(text):
    """
    Gemini APIã§ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„

    å¼•æ•°:
        text: è¦ç´„ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ

    æˆ»ã‚Šå€¤:
        è¦ç´„ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ–‡å­—åˆ—ï¼‰
    """
    # Gemini APIåˆæœŸåŒ–
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    model = genai.GenerativeModel("gemini-2.5-flash")

    # è¦ç´„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt = f"""ä»¥ä¸‹ã®æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ã€è¦ç´„å½¢å¼ã€‘
1. ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ï¼ˆ2-3è¡Œï¼‰
2. ä¸»è¦ãƒã‚¤ãƒ³ãƒˆï¼ˆç®‡æ¡æ›¸ãã€3-5é …ç›®ï¼‰
3. è©³ç´°ã‚µãƒãƒªãƒ¼ï¼ˆæ®µè½å½¢å¼ï¼‰

ã€æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã€‘
{text}
"""

    response = model.generate_content(prompt)
    return response.text

def save_text(text, output_path):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

    å¼•æ•°:
        text: ä¿å­˜ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(text)
    print(f"âœ… ä¿å­˜å®Œäº†: {output_path}")

def save_markdown(transcription, summary, output_path):
    """
    æ–‡å­—èµ·ã“ã—ã¨è¦ç´„ã‚’Markdownå½¢å¼ã§ä¿å­˜

    å¼•æ•°:
        transcription: æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆ
        summary: è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ
        output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    # Markdownãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    markdown_content = f"""# æ–‡å­—èµ·ã“ã—çµæœ

## è¦ç´„

{summary}

---

## å…¨æ–‡

{transcription}
"""

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(markdown_content)
    print(f"âœ… Markdownä¿å­˜å®Œäº†: {output_path}")

def main():
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãƒã‚§ãƒƒã‚¯
    if len(sys.argv) < 2:
        print("ä½¿ã„æ–¹: python transcribe_api.py <éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹>")
        sys.exit(1)

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å–å¾—
    audio_path = sys.argv[1]

    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    if not os.path.exists(audio_path):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {audio_path}")
        sys.exit(1)

    print(f"ğŸ™ï¸ æ–‡å­—èµ·ã“ã—é–‹å§‹: {audio_path}")

    # æ–‡å­—èµ·ã“ã—å®Ÿè¡Œï¼ˆGemini Audio API + è©±è€…è­˜åˆ¥ï¼‰
    text, speakers = transcribe_audio(audio_path)

    # è©±è€…æƒ…å ±è¡¨ç¤º
    if speakers:
        print(f"ğŸ“¢ æ¤œå‡ºã•ã‚ŒãŸè©±è€…: {len(speakers)}å")
        for speaker in speakers:
            print(f"  - {speaker}")

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
    base_path = audio_path.rsplit(".", 1)[0]
    txt_path = base_path + "_transcription.txt"
    md_path = base_path + "_summary.md"

    # ãƒ†ã‚­ã‚¹ãƒˆä¿å­˜
    save_text(text, txt_path)

    print("ğŸ“ è¦ç´„ç”Ÿæˆä¸­...")

    # è¦ç´„ç”Ÿæˆ
    summary = summarize_text(text)

    # Markdownä¿å­˜
    save_markdown(text, summary, md_path)

    print("ğŸ‰ å®Œäº†!")

if __name__ == "__main__":
    main()
