#!/usr/bin/env python3
"""
Phase 6-3 Stage 1: Vector Index Builder
ChromaDBã‚’ä½¿ç”¨ã—ã¦æ–‡å­—èµ·ã“ã—ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰

æ©Ÿèƒ½:
- enhanced JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿
- OpenAI Embeddings API (text-embedding-3-small) ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–
- ChromaDBã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãã§ä¿å­˜
- ãƒˆãƒ”ãƒƒã‚¯ã€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å«ã‚€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
"""

import json
import os
import sys
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
import chromadb
from chromadb.config import Settings

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# OpenAI API ã‚­ãƒ¼ç¢ºèª
if not os.getenv("OPENAI_API_KEY"):
    print("âŒ Error: OPENAI_API_KEY not found in environment variables")
    sys.exit(1)


class VectorIndexBuilder:
    """æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã‚¯ãƒ©ã‚¹"""

    def __init__(self, chroma_path: str = "chroma_db"):
        """
        Args:
            chroma_path: ChromaDBã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.chroma_path = Path(chroma_path)
        self.chroma_path.mkdir(parents=True, exist_ok=True)

        # ChromaDB ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        self.client = chromadb.PersistentClient(
            path=str(self.chroma_path),
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )

        # OpenAI Embeddings åˆæœŸåŒ– (text-embedding-3-small)
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )

        print(f"âœ… ChromaDB initialized at: {self.chroma_path}")
        print(f"âœ… Using OpenAI model: text-embedding-3-small")

    def load_enhanced_json(self, json_path: str) -> Dict[str, Any]:
        """enhanced JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"âœ… Loaded JSON: {json_path}")
        print(f"   Segments: {len(data.get('segments', []))}")
        print(f"   Topics: {len(data.get('topics', []))}")
        print(f"   Entities: {sum(len(v) for v in data.get('entities', {}).values())}")

        return data

    def prepare_documents(self, data: Dict[str, Any]) -> tuple[List[str], List[Dict[str, Any]]]:
        """
        ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™

        Returns:
            texts: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            metadatas: å„ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾å¿œã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        segments = data.get('segments', [])
        topics = data.get('topics', [])
        entities = data.get('entities', {})
        file_metadata = data.get('metadata', {})

        # ãƒˆãƒ”ãƒƒã‚¯IDã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯åã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ
        topic_map = {topic['id']: topic['name'] for topic in topics}

        texts = []
        metadatas = []

        for segment in segments:
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
            text = segment.get('text', '').strip()
            if not text:
                continue

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
            metadata = {
                'segment_id': segment.get('id'),
                'start_time': segment.get('start'),
                'end_time': segment.get('end'),
                'file_name': file_metadata.get('file', {}).get('file_name', ''),
                'duration': segment.get('end', 0) - segment.get('start', 0),
            }

            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒˆãƒ”ãƒƒã‚¯
            segment_topics = segment.get('topics', [])
            if segment_topics:
                topic_names = [topic_map.get(tid, tid) for tid in segment_topics]
                metadata['topics'] = ', '.join(topic_names)
            else:
                metadata['topics'] = ''

            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ”ãƒƒã‚¯ï¼ˆå…¨ä½“ã®ãƒˆãƒ”ãƒƒã‚¯ï¼‰
            if topics:
                global_topic_names = [t['name'] for t in topics[:3]]  # ä¸Šä½3ã¤
                metadata['global_topics'] = ', '.join(global_topic_names)
            else:
                metadata['global_topics'] = ''

            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æƒ…å ±
            metadata['people'] = ', '.join(entities.get('people', [])[:5])  # ä¸Šä½5å
            metadata['organizations'] = ', '.join(entities.get('organizations', [])[:5])
            metadata['dates'] = ', '.join(entities.get('dates', [])[:5])

            texts.append(text)
            metadatas.append(metadata)

        print(f"âœ… Prepared {len(texts)} documents for vectorization")

        return texts, metadatas

    def build_index(self, texts: List[str], metadatas: List[Dict[str, Any]],
                   collection_name: str = "transcripts") -> None:
        """
        ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã—ã¦ChromaDBã«ä¿å­˜

        Args:
            texts: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            metadatas: å„ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾å¿œã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            collection_name: ChromaDBã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        """
        print(f"\nğŸ”„ Building vector index...")
        print(f"   Collection: {collection_name}")
        print(f"   Documents: {len(texts)}")

        # æ—¢å­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¹ã‚¿ãƒ¼ãƒˆï¼‰
        try:
            self.client.delete_collection(name=collection_name)
            print(f"   Deleted existing collection: {collection_name}")
        except Exception:
            pass  # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ç„¡è¦–

        # æ–°ã—ã„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ
        collection = self.client.create_collection(
            name=collection_name,
            metadata={"description": "Voice memo transcription segments with embeddings"}
        )

        # ãƒãƒƒãƒå‡¦ç†ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨ä¿å­˜
        batch_size = 100
        total_batches = (len(texts) + batch_size - 1) // batch_size

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_metadatas = metadatas[i:i + batch_size]
            batch_ids = [f"seg_{meta['segment_id']}" for meta in batch_metadatas]

            # OpenAI Embeddings APIã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            print(f"   Batch {i//batch_size + 1}/{total_batches}: Generating embeddings...")
            batch_embeddings = self.embeddings.embed_documents(batch_texts)

            # ChromaDBã«ä¿å­˜
            collection.add(
                documents=batch_texts,
                embeddings=batch_embeddings,
                metadatas=batch_metadatas,
                ids=batch_ids
            )

        print(f"âœ… Vector index built successfully")
        print(f"   Total documents: {collection.count()}")
        print(f"   Stored in: {self.chroma_path / collection_name}")

    def verify_index(self, collection_name: str = "transcripts") -> None:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ¤œè¨¼ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªå®Ÿè¡Œï¼‰"""
        print(f"\nğŸ” Verifying index...")

        collection = self.client.get_collection(name=collection_name)

        # ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒª
        test_query = "ãƒ†ã‚¹ãƒˆ"
        print(f"   Test query: '{test_query}'")

        query_embedding = self.embeddings.embed_query(test_query)

        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=3
        )

        print(f"\nâœ… Top 3 results:")
        for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):
            print(f"\n   {i}. Text: {doc[:100]}...")
            print(f"      Segment ID: {metadata.get('segment_id')}")
            print(f"      Time: {metadata.get('start_time'):.2f}s - {metadata.get('end_time'):.2f}s")
            print(f"      Topics: {metadata.get('topics', 'N/A')}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    if len(sys.argv) < 2:
        print("Usage: python build_vector_index.py <enhanced_json_path>")
        print("Example: python build_vector_index.py 'downloads/Test Recording_20min_enhanced.json'")
        sys.exit(1)

    json_path = sys.argv[1]

    if not os.path.exists(json_path):
        print(f"âŒ Error: File not found: {json_path}")
        sys.exit(1)

    print("=" * 70)
    print("Phase 6-3 Stage 1: Vector Index Builder")
    print("=" * 70)

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ“ãƒ«ãƒ€ãƒ¼åˆæœŸåŒ–
    builder = VectorIndexBuilder(chroma_path="chroma_db")

    # JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    data = builder.load_enhanced_json(json_path)

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæº–å‚™
    texts, metadatas = builder.prepare_documents(data)

    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
    file_name = Path(json_path).stem
    collection_name = f"transcripts_{file_name.replace(' ', '_').replace('(', '').replace(')', '')}"
    builder.build_index(texts, metadatas, collection_name=collection_name)

    # æ¤œè¨¼
    builder.verify_index(collection_name=collection_name)

    print("\n" + "=" * 70)
    print("âœ… Vector index building completed successfully!")
    print("=" * 70)


if __name__ == "__main__":
    main()
