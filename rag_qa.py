#!/usr/bin/env python3
"""
Phase 7 Stage 7-2: RAG Q&A System (Gemini Embeddings)
ChromaDBã¨Gemini APIã‚’ä½¿ç”¨ã—ãŸRAG (Retrieval Augmented Generation) Q&Aã‚·ã‚¹ãƒ†ãƒ 

æ©Ÿèƒ½:
- è‡ªç„¶è¨€èªè³ªå•ã«å¯¾ã—ã¦ã€æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢ã—ã¦å›ç­”ç”Ÿæˆ
- å¼•ç”¨å…ƒã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã€ãƒˆãƒ”ãƒƒã‚¯ï¼‰ã‚’è¡¨ç¤º
- è¤‡æ•°ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹çµ±åˆ
- å›ç­”ã®ä¿¡é ¼æ€§ã¨é–¢é€£æ€§ã‚’è©•ä¾¡
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
import chromadb
from chromadb.config import Settings
import google.generativeai as genai

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# Gemini API ã‚­ãƒ¼ç¢ºèª
if not os.getenv("GEMINI_API_KEY"):
    print("âŒ Error: GEMINI_API_KEY not found in environment variables")
    sys.exit(1)


class RAGQASystem:
    """RAG Q&Aã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒ©ã‚¹"""

    def __init__(self, chroma_path: str = "chroma_db"):
        """
        Args:
            chroma_path: ChromaDBã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.chroma_path = Path(chroma_path)

        if not self.chroma_path.exists():
            raise FileNotFoundError(f"ChromaDB not found at: {self.chroma_path}")

        # ChromaDB ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        self.client = chromadb.PersistentClient(
            path=str(self.chroma_path),
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=False
            )
        )

        # Gemini API åˆæœŸåŒ–
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        self.llm = genai.GenerativeModel("gemini-2.5-pro")

        print(f"âœ… RAG Q&A System initialized")
        print(f"   ChromaDB: {self.chroma_path}")
        print(f"   Embeddings: text-embedding-004 (Gemini)")
        print(f"   LLM: gemini-2.5-pro")

    def retrieve_context(
        self,
        query: str,
        collection_name: str,
        n_results: int = 5
    ) -> List[Dict[str, Any]]:
        """
        è³ªå•ã«é–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ChromaDBã‹ã‚‰æ¤œç´¢

        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            collection_name: ChromaDBã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
            n_results: æ¤œç´¢ã™ã‚‹çµæœæ•°

        Returns:
            é–¢é€£ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        print(f"\nğŸ” Retrieving context for: '{query}'")

        collection = self.client.get_collection(name=collection_name)

        # ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦æ¤œç´¢ï¼ˆGemini Embeddings APIï¼‰
        result = genai.embed_content(
            model="models/text-embedding-004",
            content=query,
            task_type="retrieval_query"
        )
        query_embedding = result['embedding']

        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )

        # çµæœã‚’æ•´å½¢
        contexts = []
        for doc, metadata, distance in zip(
            results['documents'][0],
            results['metadatas'][0],
            results['distances'][0]
        ):
            similarity_score = 1 / (1 + distance)

            context = {
                "text": doc,
                "metadata": metadata,
                "similarity_score": similarity_score,
                "distance": distance
            }
            contexts.append(context)

        print(f"   Retrieved {len(contexts)} relevant segments")

        return contexts

    def generate_answer(
        self,
        query: str,
        contexts: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        æ¤œç´¢ã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å…ƒã«Gemini APIã§å›ç­”ç”Ÿæˆ

        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            contexts: æ¤œç´¢ã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ

        Returns:
            å›ç­”ã¨å¼•ç”¨æƒ…å ±
        """
        print(f"\nğŸ¤– Generating answer with Gemini...")

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        context_parts = []
        for i, ctx in enumerate(contexts):
            meta = ctx['metadata']
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¡¨ç¤ºï¼ˆstart_timeãŒã‚ã‚Œã°ãã‚Œã‚’ã€ãªã‘ã‚Œã°timestampã‚’ä½¿ç”¨ï¼‰
            if meta.get('start_time') is not None:
                time_info = f"é–‹å§‹æ™‚åˆ»: {meta.get('start_time'):.2f}ç§’"
            elif meta.get('timestamp'):
                time_info = f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {meta.get('timestamp')}"
            else:
                time_info = "æ™‚åˆ»: ä¸æ˜"

            # è©±è€…æƒ…å ±ãŒã‚ã‚Œã°è¿½åŠ 
            speaker_info = f", è©±è€…: {meta.get('speaker')}" if meta.get('speaker') else ""

            part = (
                f"[ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ {i+1}] ({time_info}{speaker_info})\n"
                f"ãƒˆãƒ”ãƒƒã‚¯: {meta.get('topics', 'ä¸æ˜')}\n"
                f"å†…å®¹: {ctx['text']}"
            )
            context_parts.append(part)

        context_text = "\n\n---\n\n".join(context_parts)

        # Gemini APIã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""ä»¥ä¸‹ã®æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªæŒ‡ç¤ºã€‘
1. å¿…ãšæä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„
2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã§ã¯å›ç­”ã§ãã¾ã›ã‚“ã€ã¨æ˜è¨˜ã—ã¦ãã ã•ã„
3. å›ç­”ã«ã¯å…·ä½“çš„ãªã‚»ã‚°ãƒ¡ãƒ³ãƒˆç•ªå·ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ï¼ˆä¾‹: [ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ 1]ï¼‰
4. å›ç­”ã¯ç°¡æ½”ã‹ã¤æ­£ç¢ºã«ã€æ—¥æœ¬èªã§è¨˜è¿°ã—ã¦ãã ã•ã„
5. è¤‡æ•°ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰æƒ…å ±ã‚’çµ±åˆã™ã‚‹å ´åˆã¯ã€ã™ã¹ã¦ã®é–¢é€£ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å¼•ç”¨ã—ã¦ãã ã•ã„

ã€æ–‡å­—èµ·ã“ã—ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘
{context_text}

ã€è³ªå•ã€‘
{query}

ã€å›ç­”ã€‘
"""

        # Gemini APIã§å›ç­”ç”Ÿæˆ
        response = self.llm.generate_content(prompt)
        answer_text = response.text.strip()

        print(f"   Answer generated ({len(answer_text)} characters)")

        return {
            "query": query,
            "answer": answer_text,
            "contexts": contexts,
            "num_contexts_used": len(contexts)
        }

    def ask(
        self,
        query: str,
        collection_name: str,
        n_contexts: int = 5
    ) -> Dict[str, Any]:
        """
        è³ªå•ã«å›ç­”ã™ã‚‹ï¼ˆãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼‰

        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            collection_name: ChromaDBã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
            n_contexts: ä½¿ç”¨ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ•°

        Returns:
            å›ç­”ã¨å¼•ç”¨æƒ…å ±
        """
        # 1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢
        contexts = self.retrieve_context(query, collection_name, n_contexts)

        # 2. å›ç­”ç”Ÿæˆ
        result = self.generate_answer(query, contexts)

        return result

    def display_answer(self, result: Dict[str, Any]) -> None:
        """å›ç­”ã‚’è¦‹ã‚„ã™ãè¡¨ç¤º"""
        print(f"\n{'='*70}")
        print(f"â“ Question: {result['query']}")
        print(f"{'='*70}")

        print(f"\nğŸ’¡ Answer:\n")
        print(result['answer'])

        print(f"\n{'â”€'*70}")
        print(f"ğŸ“š Sources ({result['num_contexts_used']} segments used):")
        print(f"{'â”€'*70}")

        for i, ctx in enumerate(result['contexts'], 1):
            meta = ctx['metadata']

            print(f"\n[ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ {i}] (é¡ä¼¼åº¦: {ctx['similarity_score']:.4f})")
            print(f"â±ï¸  æ™‚åˆ»: {meta.get('start_time', 'N/A'):.2f}s - {meta.get('end_time', 'N/A'):.2f}s")
            print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {meta.get('file_name', 'N/A')}")

            if meta.get('topics'):
                print(f"ğŸ·ï¸  ãƒˆãƒ”ãƒƒã‚¯: {meta['topics']}")

            if meta.get('people'):
                print(f"ğŸ‘¥ äººç‰©: {meta['people']}")

            text = ctx['text']
            if len(text) > 150:
                text = text[:150] + "..."
            print(f"ğŸ“ å†…å®¹: {text}")

    def batch_ask(
        self,
        questions: List[str],
        collection_name: str,
        n_contexts: int = 5
    ) -> List[Dict[str, Any]]:
        """
        è¤‡æ•°ã®è³ªå•ã«ä¸€æ‹¬ã§å›ç­”

        Args:
            questions: è³ªå•ã®ãƒªã‚¹ãƒˆ
            collection_name: ChromaDBã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
            n_contexts: ä½¿ç”¨ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ•°

        Returns:
            å›ç­”çµæœã®ãƒªã‚¹ãƒˆ
        """
        results = []

        for i, question in enumerate(questions, 1):
            print(f"\n{'='*70}")
            print(f"Question {i}/{len(questions)}")
            print(f"{'='*70}")

            result = self.ask(question, collection_name, n_contexts)
            self.display_answer(result)

            results.append(result)

        return results


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 70)
    print("Phase 6-3 Stage 2: RAG Q&A System")
    print("=" * 70)

    # RAG Q&Aã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    rag_system = RAGQASystem(chroma_path="chroma_db")

    # åˆ©ç”¨å¯èƒ½ãªã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è¡¨ç¤º
    collections = [col.name for col in rag_system.client.list_collections()]
    print(f"\nğŸ“š Available collections:")
    for i, col in enumerate(collections, 1):
        print(f"   {i}. {col}")

    if not collections:
        print("âŒ No collections found. Please run build_vector_index.py first.")
        sys.exit(1)

    # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³é¸æŠ
    if len(sys.argv) > 1:
        collection_name = sys.argv[1]
    else:
        collection_name = collections[0]

    print(f"\nâœ… Using collection: {collection_name}")

    # ã‚µãƒ³ãƒ—ãƒ«è³ªå•
    sample_questions = [
        "ã“ã®ä¼šè©±ã®ä¸»ãªãƒˆãƒ”ãƒƒã‚¯ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "è©±è€…ã¯å°±è·æ´»å‹•ã«ã¤ã„ã¦ã©ã®ã‚ˆã†ã«è€ƒãˆã¦ã„ã¾ã™ã‹ï¼Ÿ",
        "å–¶æ¥­ã¨AIã«ã¤ã„ã¦ã©ã®ã‚ˆã†ãªè­°è«–ãŒã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"
    ]

    # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã¾ãŸã¯ã‚µãƒ³ãƒ—ãƒ«è³ªå•ãƒ¢ãƒ¼ãƒ‰
    if len(sys.argv) > 2 and sys.argv[2] == "--interactive":
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
        print("\n" + "="*70)
        print("Interactive Q&A Mode (type 'exit' to quit)")
        print("="*70)

        while True:
            try:
                user_question = input("\nâ“ Enter your question: ").strip()

                if user_question.lower() in ['exit', 'quit', 'q']:
                    print("ğŸ‘‹ Goodbye!")
                    break

                if not user_question:
                    continue

                result = rag_system.ask(user_question, collection_name)
                rag_system.display_answer(result)

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Goodbye!")
                break
            except Exception as e:
                print(f"âŒ Error: {e}")

    else:
        # ã‚µãƒ³ãƒ—ãƒ«è³ªå•ãƒ¢ãƒ¼ãƒ‰
        print("\n" + "="*70)
        print("Sample Questions Demo")
        print("="*70)

        results = rag_system.batch_ask(sample_questions, collection_name)

        print("\n" + "="*70)
        print(f"âœ… Demo completed: {len(results)} questions answered")
        print("="*70)
        print("\nTip: Run with --interactive flag for interactive Q&A mode")


if __name__ == "__main__":
    main()
