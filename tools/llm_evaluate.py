#!/usr/bin/env python3
"""
LLMè©•ä¾¡: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å“è³ªæ¯”è¼ƒ

LLMã‚’ä½¿ã£ã¦2ã¤ã®å‡ºåŠ›ã‚’æ¯”è¼ƒè©•ä¾¡
"""

import json
import sys
import os
from pathlib import Path
import google.generativeai as genai
from datetime import datetime
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# Gemini API Keyé¸æŠžï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç„¡æ–™æž ï¼‰
USE_PAID_TIER = os.getenv("USE_PAID_TIER", "false").lower() == "true"
if USE_PAID_TIER:
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY_PAID")
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY_PAID not set but USE_PAID_TIER=true")
    print("â„¹ï¸  Using PAID tier API key")
else:
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY_FREE")
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY_FREE not set")
    print("â„¹ï¸  Using FREE tier API key")

genai.configure(api_key=GEMINI_API_KEY)

# Gemini 2.5 Pro
MODEL_NAME = 'gemini-2.5-pro'

def llm_compare_summaries(original_segments, baseline_summary, new_pipeline_summary):
    """
    LLMã‚’ä½¿ã£ã¦2ã¤ã®è¦ç´„ã‚’æ¯”è¼ƒè©•ä¾¡
    """
    model = genai.GenerativeModel(MODEL_NAME)

    # å…ƒã®ä¼šè©±ï¼ˆæœ€åˆã®50ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼‰
    sample_size = min(50, len(original_segments))
    original_sample = "\n".join([
        f"{seg.get('speaker', 'Speaker')}: {seg['text']}"
        for seg in original_segments[:sample_size]
    ])

    # è¦ç´„ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®5ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼‰
    baseline_sample = "\n".join([seg['text'] for seg in baseline_summary[:5]])
    new_pipeline_sample = "\n".join([seg['text'] for seg in new_pipeline_summary[:5]])

    prompt = f"""ä»¥ä¸‹ã®2ã¤ã®è¦ç´„ã‚’æ¯”è¼ƒè©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

ã€å…ƒã®ä¼šè©±ï¼ˆå†’é ­éƒ¨åˆ†ï¼‰ã€‘
{original_sample}

ã€è¦ç´„Aï¼ˆå¾“æ¥å‡¦ç†: è©±è€…æƒ…å ±ãªã—ï¼‰ã€‘
{baseline_sample}

ã€è¦ç´„Bï¼ˆæ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: è©±è€…æƒ…å ±ã‚ã‚Šã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¼·åŒ–ï¼‰ã€‘
{new_pipeline_sample}

ä»¥ä¸‹ã®åŸºæº–ã§1-5ç‚¹ã§è©•ä¾¡ã—ã¦ãã ã•ã„ï¼š
1. æƒ…å ±ã®æ­£ç¢ºæ€§: é‡è¦ãªæƒ…å ±ã‚’æ­£ç¢ºã«ä¿æŒã—ã¦ã„ã‚‹ã‹
2. æ–‡è„ˆç†è§£åº¦: è©±è€…ã®æ„å›³ã‚„æ„æ€æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹ãŒæ˜Žç¢ºã‹
3. æœ‰ç”¨æ€§: å¾Œã§è¦‹è¿”ã—ãŸã¨ãã«æœ‰ç”¨ã‹
4. ç°¡æ½”æ€§: å†—é•·æ€§ã‚’æŽ’é™¤ã—ã€ç°¡æ½”ã«ã¾ã¨ã¾ã£ã¦ã„ã‚‹ã‹

ã¾ãŸã€ã©ã¡ã‚‰ã®è¦ç´„ãŒå„ªã‚Œã¦ã„ã‚‹ã‹ã€ãã®ç†ç”±ã‚‚èª¬æ˜Žã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›žç­”ã—ã¦ãã ã•ã„:
{{
  "baseline_scores": {{
    "accuracy": 1-5,
    "context_understanding": 1-5,
    "usefulness": 1-5,
    "conciseness": 1-5
  }},
  "new_pipeline_scores": {{
    "accuracy": 1-5,
    "context_understanding": 1-5,
    "usefulness": 1-5,
    "conciseness": 1-5
  }},
  "winner": "baseline" or "new_pipeline" or "tie",
  "reasoning": "åˆ¤æ–­ç†ç”±ã‚’è©³ã—ãèª¬æ˜Ž"
}}"""

    response = model.generate_content(
        prompt,
        generation_config={
            'temperature': 0.1,
            'response_mime_type': 'application/json'
        }
    )

    return json.loads(response.text)

def llm_compare_filenames(original_filename, baseline_filename, new_pipeline_filename, topics_baseline, topics_new):
    """
    LLMã‚’ä½¿ã£ã¦2ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¯”è¼ƒè©•ä¾¡
    """
    model = genai.GenerativeModel(MODEL_NAME)

    prompt = f"""ä»¥ä¸‹ã®2ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¯”è¼ƒè©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

ã€å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã€‘
{original_filename}

ã€è¦ç´„ã«ä½¿ç”¨ã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ã€‘
å¾“æ¥å‡¦ç†: {', '.join(topics_baseline[:5])}
æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: {', '.join(topics_new[:5])}

ã€ãƒ•ã‚¡ã‚¤ãƒ«åAï¼ˆå¾“æ¥å‡¦ç†ï¼‰ã€‘
{baseline_filename}

ã€ãƒ•ã‚¡ã‚¤ãƒ«åBï¼ˆæ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼‰ã€‘
{new_pipeline_filename}

ä»¥ä¸‹ã®åŸºæº–ã§1-5ç‚¹ã§è©•ä¾¡ã—ã¦ãã ã•ã„ï¼š
1. æƒ…å ±é‡: æœ‰ç”¨ãªæƒ…å ±ãŒã©ã‚Œã ã‘å«ã¾ã‚Œã¦ã„ã‚‹ã‹
2. æ¤œç´¢æ€§: å¾Œã§æ¤œç´¢ã—ã‚„ã™ã„ã‹
3. å¯èª­æ€§: èª­ã¿ã‚„ã™ãç†è§£ã—ã‚„ã™ã„ã‹
4. é©åˆ‡æ€§: å…ƒã®å†…å®¹ã‚’é©åˆ‡ã«è¡¨ç¾ã—ã¦ã„ã‚‹ã‹

ã¾ãŸã€ã©ã¡ã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«åãŒå„ªã‚Œã¦ã„ã‚‹ã‹ã€ãã®ç†ç”±ã‚‚èª¬æ˜Žã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›žç­”ã—ã¦ãã ã•ã„:
{{
  "baseline_scores": {{
    "information_richness": 1-5,
    "searchability": 1-5,
    "readability": 1-5,
    "appropriateness": 1-5
  }},
  "new_pipeline_scores": {{
    "information_richness": 1-5,
    "searchability": 1-5,
    "readability": 1-5,
    "appropriateness": 1-5
  }},
  "winner": "baseline" or "new_pipeline" or "tie",
  "reasoning": "åˆ¤æ–­ç†ç”±ã‚’è©³ã—ãèª¬æ˜Ž"
}}"""

    response = model.generate_content(
        prompt,
        generation_config={
            'temperature': 0.1,
            'response_mime_type': 'application/json'
        }
    )

    return json.loads(response.text)

def calculate_overall_scores(summary_eval, filename_eval):
    """
    ç·åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    """
    # è¦ç´„ã‚¹ã‚³ã‚¢ã®å¹³å‡
    baseline_summary_avg = sum(summary_eval['baseline_scores'].values()) / len(summary_eval['baseline_scores'])
    new_pipeline_summary_avg = sum(summary_eval['new_pipeline_scores'].values()) / len(summary_eval['new_pipeline_scores'])

    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚¹ã‚³ã‚¢ã®å¹³å‡
    baseline_filename_avg = sum(filename_eval['baseline_scores'].values()) / len(filename_eval['baseline_scores'])
    new_pipeline_filename_avg = sum(filename_eval['new_pipeline_scores'].values()) / len(filename_eval['new_pipeline_scores'])

    # ç·åˆã‚¹ã‚³ã‚¢ï¼ˆè¦ç´„70%ã€ãƒ•ã‚¡ã‚¤ãƒ«å30%ã®é‡ã¿ä»˜ã‘ï¼‰
    baseline_overall = baseline_summary_avg * 0.7 + baseline_filename_avg * 0.3
    new_pipeline_overall = new_pipeline_summary_avg * 0.7 + new_pipeline_filename_avg * 0.3

    improvement = new_pipeline_overall - baseline_overall

    return {
        'baseline_overall': baseline_overall,
        'new_pipeline_overall': new_pipeline_overall,
        'improvement': improvement,
        'improvement_percentage': (improvement / baseline_overall * 100) if baseline_overall > 0 else 0
    }

def main():
    if len(sys.argv) < 3:
        print("Usage: python llm_evaluate.py <baseline_result.json> <new_pipeline_result.json>")
        sys.exit(1)

    baseline_file = sys.argv[1]
    new_pipeline_file = sys.argv[2]

    if not os.path.exists(baseline_file):
        print(f"Error: Baseline file not found: {baseline_file}")
        sys.exit(1)

    if not os.path.exists(new_pipeline_file):
        print(f"Error: New pipeline file not found: {new_pipeline_file}")
        sys.exit(1)

    print("="*80)
    print("ðŸ¤– LLM Evaluation: Baseline vs New Pipeline")
    print("="*80)

    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    print("\nðŸ“‚ Loading files...")
    with open(baseline_file, 'r', encoding='utf-8') as f:
        baseline = json.load(f)

    with open(new_pipeline_file, 'r', encoding='utf-8') as f:
        new_pipeline = json.load(f)

    # è¦ç´„ã®æ¯”è¼ƒ
    print("\nðŸ” Evaluating summaries with LLM...")
    summary_eval = llm_compare_summaries(
        baseline['segments'],
        baseline['summarized_segments'],
        new_pipeline['summarized_segments']
    )

    print(f"âœ… Summary evaluation complete")
    print(f"   Winner: {summary_eval['winner']}")

    # ãƒ•ã‚¡ã‚¤ãƒ«åã®æ¯”è¼ƒ
    print("\nðŸ” Evaluating filenames with LLM...")
    filename_eval = llm_compare_filenames(
        baseline['metadata']['original_filename'],
        baseline['generated_filename'],
        new_pipeline['metadata']['optimal_filename']['filename'],
        baseline.get('topics', []),
        new_pipeline.get('topics_entities', {}).get('topics', [])
    )

    print(f"âœ… Filename evaluation complete")
    print(f"   Winner: {filename_eval['winner']}")

    # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
    print("\nðŸ” Calculating overall scores...")
    overall_scores = calculate_overall_scores(summary_eval, filename_eval)

    # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
    report = {
        'metadata': {
            'evaluation_date': datetime.now().isoformat(),
            'evaluation_method': 'llm_comparison',
            'model': MODEL_NAME
        },
        'summary_evaluation': summary_eval,
        'filename_evaluation': filename_eval,
        'overall_scores': overall_scores,
        'conclusion': {
            'winner': 'new_pipeline' if overall_scores['improvement'] > 0.5 else 'baseline' if overall_scores['improvement'] < -0.5 else 'tie',
            'improvement_summary': f"æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ç·åˆã‚¹ã‚³ã‚¢ã§{overall_scores['improvement']:.2f}ç‚¹ï¼ˆ{overall_scores['improvement_percentage']:.1f}%ï¼‰ã®æ”¹å–„"
        }
    }

    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    output_file = "evaluation_report_llm.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nðŸ“Š LLM Evaluation Report saved to: {output_file}")

    # ã‚µãƒžãƒªãƒ¼è¡¨ç¤º
    print("\n" + "="*80)
    print("ðŸ“ˆ LLM Evaluation Summary")
    print("="*80)

    print(f"\nã€è¦ç´„ã®æ¯”è¼ƒã€‘")
    print(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å¹³å‡: {sum(summary_eval['baseline_scores'].values()) / len(summary_eval['baseline_scores']):.2f}/5.00")
    print(f"  æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¹³å‡: {sum(summary_eval['new_pipeline_scores'].values()) / len(summary_eval['new_pipeline_scores']):.2f}/5.00")
    print(f"  Winner: {summary_eval['winner']}")

    print(f"\nã€ãƒ•ã‚¡ã‚¤ãƒ«åã®æ¯”è¼ƒã€‘")
    print(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å¹³å‡: {sum(filename_eval['baseline_scores'].values()) / len(filename_eval['baseline_scores']):.2f}/5.00")
    print(f"  æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¹³å‡: {sum(filename_eval['new_pipeline_scores'].values()) / len(filename_eval['new_pipeline_scores']):.2f}/5.00")
    print(f"  Winner: {filename_eval['winner']}")

    print(f"\nã€ç·åˆè©•ä¾¡ã€‘")
    print(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç·åˆ: {overall_scores['baseline_overall']:.2f}/5.00")
    print(f"  æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç·åˆ: {overall_scores['new_pipeline_overall']:.2f}/5.00")
    print(f"  æ”¹å–„: {overall_scores['improvement']:+.2f}ç‚¹ ({overall_scores['improvement_percentage']:+.1f}%)")
    print(f"  Winner: {report['conclusion']['winner']}")

    print("\nâœ… LLM evaluation complete!")

if __name__ == '__main__':
    main()
