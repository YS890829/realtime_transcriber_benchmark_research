#!/usr/bin/env python3
"""
çµ±åˆæ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: å…¨æ¤œè¨¼ãƒ—ãƒ­ã‚»ã‚¹ã‚’è‡ªå‹•å®Ÿè¡Œ

1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‡¦ç†å®Ÿè¡Œ
2. æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œï¼ˆæ—¢å­˜çµæžœã‚‚åˆ©ç”¨å¯èƒ½ï¼‰
3. è‡ªå‹•è©•ä¾¡
4. LLMè©•ä¾¡
5. çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import sys
import os
import json
from pathlib import Path
from datetime import datetime

# å„ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from baseline_pipeline import baseline_pipeline
from evaluate_accuracy import evaluate_baseline, evaluate_new_pipeline, compare_metrics, generate_report as generate_auto_report
from llm_evaluate import llm_compare_summaries, llm_compare_filenames, calculate_overall_scores

def check_new_pipeline_result(structured_file):
    """
    æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®çµæžœãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    """
    # _structured_final.json ã‚’æŽ¢ã™
    final_file = structured_file.replace('_structured.json', '_structured_final.json')

    if os.path.exists(final_file):
        return final_file
    else:
        return None

def generate_markdown_report(auto_report, llm_report, output_file):
    """
    çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownå½¢å¼ã§ç”Ÿæˆ
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# ç²¾åº¦æ”¹å–„æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ\n\n")

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        f.write(f"**æ¤œè¨¼æ—¥æ™‚**: {auto_report['metadata']['evaluation_date']}\n\n")

        # ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒžãƒªãƒ¼
        f.write("## ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒžãƒªãƒ¼\n\n")
        f.write("æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆè©±è€…æŽ¨è«– â†’ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ãè¦ç´„ â†’ æœ€é©ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆï¼‰ã¨å¾“æ¥å‡¦ç†ã‚’æ¯”è¼ƒã—ãŸçµæžœ:\n\n")

        for improvement in auto_report['summary']['key_improvements']:
            f.write(f"- âœ… {improvement}\n")

        f.write(f"\n**LLMç·åˆè©•ä¾¡**: ")
        improvement = llm_report['overall_scores']['improvement']
        if improvement > 0.5:
            f.write(f"æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒæ˜Žç¢ºã«å„ªã‚Œã¦ã„ã‚‹ï¼ˆ+{improvement:.2f}ç‚¹ï¼‰\n\n")
        elif improvement > 0:
            f.write(f"æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒã‚„ã‚„å„ªã‚Œã¦ã„ã‚‹ï¼ˆ+{improvement:.2f}ç‚¹ï¼‰\n\n")
        else:
            f.write(f"ã»ã¼åŒç­‰\n\n")

        # è‡ªå‹•è©•ä¾¡çµæžœ
        f.write("## è‡ªå‹•è©•ä¾¡çµæžœ\n\n")
        f.write("### ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆå¾“æ¥å‡¦ç†ï¼‰\n\n")
        f.write(f"- ãƒˆãƒ”ãƒƒã‚¯æ•°: {auto_report['baseline']['topic_count']}\n")
        f.write(f"- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¿æŒçŽ‡: {auto_report['baseline']['keyword_retention_rate']*100:.1f}%\n")
        f.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«å: {auto_report['baseline']['filename']}\n")
        f.write(f"- æƒ…å ±å¯†åº¦: {auto_report['baseline']['filename_info_density']}\n\n")

        f.write("### æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n\n")
        f.write(f"- è©±è€…è­˜åˆ¥ä¿¡é ¼åº¦: {auto_report['new_pipeline']['speaker_identification']['confidence']}\n")
        f.write(f"- ãƒˆãƒ”ãƒƒã‚¯æ•°: {auto_report['new_pipeline']['topic_count']}\n")
        f.write(f"- ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ•°: {auto_report['new_pipeline']['entity_count']}\n")
        f.write(f"- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¿æŒçŽ‡: {auto_report['new_pipeline']['keyword_retention_rate']*100:.1f}%\n")
        f.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«å: {auto_report['new_pipeline']['filename']}\n")
        f.write(f"- æƒ…å ±å¯†åº¦: {auto_report['new_pipeline']['filename_info_density']}\n\n")

        f.write("### æ”¹å–„çŽ‡\n\n")
        f.write(f"- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¿æŒçŽ‡æ”¹å–„: {auto_report['comparison']['keyword_retention_improvement']*100:+.1f}%\n")
        f.write(f"- ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºæ”¹å–„: {auto_report['comparison']['topic_coverage_improvement']*100:+.1f}%\n")
        f.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«åæƒ…å ±å¯†åº¦: {auto_report['comparison']['filename_info_density_improvement']:.2f}å€\n\n")

        # LLMè©•ä¾¡çµæžœ
        f.write("## LLMè©•ä¾¡çµæžœ\n\n")

        f.write("### è¦ç´„å“è³ªã®æ¯”è¼ƒ\n\n")
        f.write("| è©•ä¾¡é …ç›® | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ |\n")
        f.write("|---------|-------------|---------------|\n")

        summary_eval = llm_report['summary_evaluation']
        for key in summary_eval['baseline_scores'].keys():
            baseline_score = summary_eval['baseline_scores'][key]
            new_score = summary_eval['new_pipeline_scores'][key]
            f.write(f"| {key} | {baseline_score:.1f}/5.0 | {new_score:.1f}/5.0 |\n")

        baseline_avg = sum(summary_eval['baseline_scores'].values()) / len(summary_eval['baseline_scores'])
        new_avg = sum(summary_eval['new_pipeline_scores'].values()) / len(summary_eval['new_pipeline_scores'])
        f.write(f"| **å¹³å‡** | **{baseline_avg:.2f}/5.0** | **{new_avg:.2f}/5.0** |\n\n")

        f.write(f"**Winner**: {summary_eval['winner']}\n\n")
        f.write(f"**ç†ç”±**: {summary_eval['reasoning']}\n\n")

        f.write("### ãƒ•ã‚¡ã‚¤ãƒ«åå“è³ªã®æ¯”è¼ƒ\n\n")
        f.write("| è©•ä¾¡é …ç›® | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ |\n")
        f.write("|---------|-------------|---------------|\n")

        filename_eval = llm_report['filename_evaluation']
        for key in filename_eval['baseline_scores'].keys():
            baseline_score = filename_eval['baseline_scores'][key]
            new_score = filename_eval['new_pipeline_scores'][key]
            f.write(f"| {key} | {baseline_score:.1f}/5.0 | {new_score:.1f}/5.0 |\n")

        baseline_avg = sum(filename_eval['baseline_scores'].values()) / len(filename_eval['baseline_scores'])
        new_avg = sum(filename_eval['new_pipeline_scores'].values()) / len(filename_eval['new_pipeline_scores'])
        f.write(f"| **å¹³å‡** | **{baseline_avg:.2f}/5.0** | **{new_avg:.2f}/5.0** |\n\n")

        f.write(f"**Winner**: {filename_eval['winner']}\n\n")
        f.write(f"**ç†ç”±**: {filename_eval['reasoning']}\n\n")

        # ç·åˆè©•ä¾¡
        f.write("## ç·åˆè©•ä¾¡\n\n")
        overall = llm_report['overall_scores']
        f.write(f"- ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç·åˆã‚¹ã‚³ã‚¢: **{overall['baseline_overall']:.2f}/5.00**\n")
        f.write(f"- æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç·åˆã‚¹ã‚³ã‚¢: **{overall['new_pipeline_overall']:.2f}/5.00**\n")
        f.write(f"- æ”¹å–„: **{overall['improvement']:+.2f}ç‚¹** ({overall['improvement_percentage']:+.1f}%)\n\n")

        # çµè«–
        f.write("## çµè«–\n\n")
        f.write(llm_report['conclusion']['improvement_summary'] + "\n\n")

        # å…·ä½“ä¾‹
        f.write("## å…·ä½“ä¾‹\n\n")
        f.write("### ãƒˆãƒ”ãƒƒã‚¯æ¯”è¼ƒ\n\n")
        f.write("**ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³**:\n")
        for topic in auto_report['baseline']['topics'][:5]:
            f.write(f"- {topic}\n")

        f.write("\n**æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**:\n")
        for topic in auto_report['new_pipeline']['topics'][:5]:
            f.write(f"- {topic}\n")

        f.write("\n### ãƒ•ã‚¡ã‚¤ãƒ«åæ¯”è¼ƒ\n\n")
        f.write(f"**ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³**: {auto_report['baseline']['filename']}\n\n")
        f.write(f"**æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**: {auto_report['new_pipeline']['filename']}\n\n")

    print(f"ðŸ“Š Markdown report saved to: {output_file}")

def run_validation(input_file, skip_baseline=False, skip_new_pipeline=False):
    """
    å…¨æ¤œè¨¼ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œ
    """
    print("="*80)
    print("ðŸš€ Starting Full Validation Process")
    print("="*80)

    # Step 1: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    if not skip_baseline:
        print("\n" + "="*80)
        print("ðŸ“ Step 1/4: Running Baseline Pipeline")
        print("="*80)
        baseline_file = baseline_pipeline(input_file)
    else:
        baseline_file = input_file.replace('_structured.json', '_baseline_result.json')
        if not os.path.exists(baseline_file):
            print(f"Error: Baseline file not found: {baseline_file}")
            sys.exit(1)
        print(f"\nâ­ï¸  Skipping baseline execution, using existing: {baseline_file}")

    # Step 2: æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæžœã®ç¢ºèª
    print("\n" + "="*80)
    print("ðŸ“ Step 2/4: Checking New Pipeline Results")
    print("="*80)

    new_pipeline_file = check_new_pipeline_result(input_file)

    if new_pipeline_file:
        print(f"âœ… Found existing new pipeline result: {new_pipeline_file}")
    elif skip_new_pipeline:
        print("âš ï¸  New pipeline file not found and skip_new_pipeline=True")
        print("Please run the new pipeline first:")
        print(f"  python run_full_pipeline.py \"{input_file}\"")
        sys.exit(1)
    else:
        print("âŒ New pipeline result not found!")
        print("Please run the new pipeline first:")
        print(f"  python run_full_pipeline.py \"{input_file}\"")
        sys.exit(1)

    # Step 3: è‡ªå‹•è©•ä¾¡
    print("\n" + "="*80)
    print("ðŸ“ Step 3/4: Automatic Evaluation")
    print("="*80)

    baseline_metrics = evaluate_baseline(baseline_file)
    new_metrics = evaluate_new_pipeline(new_pipeline_file)
    comparison = compare_metrics(baseline_metrics, new_metrics)

    auto_report = generate_auto_report(
        baseline_metrics,
        new_metrics,
        comparison,
        "evaluation_report_automatic.json"
    )

    # Step 4: LLMè©•ä¾¡
    print("\n" + "="*80)
    print("ðŸ“ Step 4/4: LLM Evaluation")
    print("="*80)

    with open(baseline_file, 'r', encoding='utf-8') as f:
        baseline = json.load(f)

    with open(new_pipeline_file, 'r', encoding='utf-8') as f:
        new_pipeline = json.load(f)

    print("ðŸ” Evaluating summaries with LLM...")
    summary_eval = llm_compare_summaries(
        baseline['segments'],
        baseline['summarized_segments'],
        new_pipeline['summarized_segments']
    )

    print("ðŸ” Evaluating filenames with LLM...")
    filename_eval = llm_compare_filenames(
        baseline['metadata']['original_filename'],
        baseline['generated_filename'],
        new_pipeline['metadata']['optimal_filename']['filename'],
        baseline.get('topics', []),
        new_pipeline.get('topics_entities', {}).get('topics', [])
    )

    overall_scores = calculate_overall_scores(summary_eval, filename_eval)

    llm_report = {
        'metadata': {
            'evaluation_date': datetime.now().isoformat(),
            'evaluation_method': 'llm_comparison',
            'model': 'gemini-2.0-flash-exp'
        },
        'summary_evaluation': summary_eval,
        'filename_evaluation': filename_eval,
        'overall_scores': overall_scores,
        'conclusion': {
            'winner': 'new_pipeline' if overall_scores['improvement'] > 0.5 else 'baseline' if overall_scores['improvement'] < -0.5 else 'tie',
            'improvement_summary': f"æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ç·åˆã‚¹ã‚³ã‚¢ã§{overall_scores['improvement']:.2f}ç‚¹ï¼ˆ{overall_scores['improvement_percentage']:.1f}%ï¼‰ã®æ”¹å–„"
        }
    }

    # LLMãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    with open("evaluation_report_llm.json", 'w', encoding='utf-8') as f:
        json.dump(llm_report, f, ensure_ascii=False, indent=2)

    # Step 5: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\n" + "="*80)
    print("ðŸ“ Generating Integrated Report")
    print("="*80)

    generate_markdown_report(auto_report, llm_report, "evaluation_report.md")

    print("\n" + "="*80)
    print("âœ… Validation Complete!")
    print("="*80)

    print("\nðŸ“Š Generated Reports:")
    print("  - evaluation_report_automatic.json (è‡ªå‹•è©•ä¾¡)")
    print("  - evaluation_report_llm.json (LLMè©•ä¾¡)")
    print("  - evaluation_report.md (çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ)")

def main():
    if len(sys.argv) < 2:
        print("Usage: python run_validation.py <structured.json> [--skip-baseline] [--skip-new-pipeline]")
        print("\nExample:")
        print("  python run_validation.py \"downloads/recording_structured.json\"")
        sys.exit(1)

    input_file = sys.argv[1]

    if not os.path.exists(input_file):
        print(f"Error: File not found: {input_file}")
        sys.exit(1)

    skip_baseline = '--skip-baseline' in sys.argv
    skip_new_pipeline = '--skip-new-pipeline' in sys.argv

    run_validation(input_file, skip_baseline, skip_new_pipeline)

if __name__ == '__main__':
    main()
