#!/usr/bin/env python3
"""
Phase 8-3: Unified Vector Index Builder
å…¨5ãƒ•ã‚¡ã‚¤ãƒ«ã‚’1ã¤ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³"transcripts_unified"ã«çµ±åˆ

æ©Ÿèƒ½:
- 5ãƒ•ã‚¡ã‚¤ãƒ«ã®_enhanced.jsonã‚’çµ±åˆ
- çµ±ä¸€ã•ã‚ŒãŸentity_idã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«source_fileè¿½åŠ 
- 1ã‚¯ã‚¨ãƒªã§5ãƒ•ã‚¡ã‚¤ãƒ«æ¨ªæ–­æ¤œç´¢
"""

import json
import os
import sys
import time
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv
import google.generativeai as genai
import chromadb
from chromadb.config import Settings

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# Gemini APIã‚­ãƒ¼é¸æŠï¼ˆFREE/PAID tierï¼‰
use_paid_tier = os.getenv("USE_PAID_TIER", "").lower() == "true"
api_key = os.getenv("GEMINI_API_KEY_PAID") if use_paid_tier else os.getenv("GEMINI_API_KEY_FREE")

if not api_key:
    print("âŒ Error: GEMINI_API_KEY_FREE or GEMINI_API_KEY_PAID not found in environment variables")
    sys.exit(1)

genai.configure(api_key=api_key)
print(f"âœ… Using Gemini API: {'PAID' if use_paid_tier else 'FREE'} tier")


class UnifiedVectorIndexBuilder:
    """çµ±åˆãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã‚¯ãƒ©ã‚¹"""

    def __init__(self, chroma_path: str = "chroma_db"):
        """
        Args:
            chroma_path: ChromaDBã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.chroma_path = Path(chroma_path)
        self.chroma_path.mkdir(parents=True, exist_ok=True)

        # ChromaDB ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        self.client = chromadb.PersistentClient(
            path=str(self.chroma_path),
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )

        print(f"âœ… ChromaDB initialized at: {self.chroma_path}")
        print(f"âœ… Using Gemini model: text-embedding-004")

    def load_enhanced_json(self, json_path: str) -> Dict[str, Any]:
        """enhanced JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        file_name = Path(json_path).stem
        print(f"âœ… Loaded: {file_name}")
        print(f"   Segments: {len(data.get('segments', []))}")
        print(f"   Topics: {len(data.get('topics', []))}")

        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£çµ±è¨ˆ
        entities = data.get('entities', {})
        people_count = len(entities.get('people', []))
        org_count = len(entities.get('organizations', []))
        print(f"   Entities: People={people_count}, Orgs={org_count}")

        return data

    def prepare_unified_documents(self, json_files: List[str]) -> tuple[List[str], List[Dict[str, Any]], List[str]]:
        """
        è¤‡æ•°ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™

        Returns:
            texts: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            metadatas: å„ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾å¿œã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            ids: å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ID
        """
        all_texts = []
        all_metadatas = []
        all_ids = []

        print(f"\nğŸ”„ Preparing documents from {len(json_files)} files...")

        for json_file in json_files:
            data = self.load_enhanced_json(json_file)

            segments = data.get('segments', [])
            topics = data.get('topics', [])
            entities = data.get('entities', {})
            file_metadata = data.get('metadata', {})

            # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆå…ƒã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«åï¼‰
            source_file = file_metadata.get('file', {}).get('file_name', Path(json_file).stem)

            # ãƒˆãƒ”ãƒƒã‚¯IDã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯åã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°
            topic_map = {topic['id']: topic['name'] for topic in topics}

            for segment in segments:
                text = segment.get('text', '').strip()
                if not text:
                    continue

                # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆIDï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ¨ªæ–­ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰
                segment_id = segment.get('id')
                file_prefix = Path(json_file).stem.replace('_structured_enhanced', '')[:20]
                unique_id = f"{file_prefix}_seg_{segment_id}"

                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                metadata = {
                    'segment_id': str(segment_id),
                    'source_file': source_file,
                    'speaker': segment.get('speaker', 'Unknown'),
                    'timestamp': segment.get('timestamp', '00:00'),
                }

                # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒˆãƒ”ãƒƒã‚¯
                segment_topics = segment.get('topics', [])
                if segment_topics:
                    topic_names = [topic_map.get(tid, tid) for tid in segment_topics]
                    metadata['segment_topics'] = ', '.join(topic_names)
                else:
                    metadata['segment_topics'] = ''

                # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ”ãƒƒã‚¯ï¼ˆå…¨ä½“ã®ãƒˆãƒ”ãƒƒã‚¯ï¼‰
                if topics:
                    global_topic_names = [t['name'] for t in topics[:3]]
                    metadata['global_topics'] = ', '.join(global_topic_names)
                else:
                    metadata['global_topics'] = ''

                # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æƒ…å ±ï¼ˆcanonical_name + entity_idä½¿ç”¨ï¼‰
                people_list = []
                for person in entities.get('people', []):
                    if isinstance(person, dict):
                        canonical = person.get('canonical_name', person.get('name', ''))
                        entity_id = person.get('entity_id', '')
                        if canonical and entity_id:
                            people_list.append(f"{canonical}({entity_id})")
                    elif isinstance(person, str):
                        people_list.append(person)

                metadata['people'] = ', '.join(people_list[:5])  # ä¸Šä½5å

                org_list = []
                for org in entities.get('organizations', []):
                    if isinstance(org, dict):
                        canonical = org.get('canonical_name', org.get('name', ''))
                        entity_id = org.get('entity_id', '')
                        if canonical and entity_id:
                            org_list.append(f"{canonical}({entity_id})")
                    elif isinstance(org, str):
                        org_list.append(org)

                metadata['organizations'] = ', '.join(org_list[:5])  # ä¸Šä½5çµ„ç¹”

                all_texts.append(text)
                all_metadatas.append(metadata)
                all_ids.append(unique_id)

        print(f"âœ… Prepared {len(all_texts)} documents from {len(json_files)} files")

        return all_texts, all_metadatas, all_ids

    def build_unified_index(self, texts: List[str], metadatas: List[Dict[str, Any]],
                           ids: List[str], collection_name: str = "transcripts_unified") -> None:
        """
        çµ±åˆãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã—ã¦ChromaDBã«ä¿å­˜

        Args:
            texts: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            metadatas: å„ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾å¿œã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            ids: å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ID
            collection_name: ChromaDBã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        """
        print(f"\nğŸ”„ Building unified vector index...")
        print(f"   Collection: {collection_name}")
        print(f"   Total documents: {len(texts)}")

        # æ—¢å­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¹ã‚¿ãƒ¼ãƒˆï¼‰
        try:
            self.client.delete_collection(name=collection_name)
            print(f"   Deleted existing collection: {collection_name}")
        except Exception:
            pass

        # æ–°ã—ã„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ
        collection = self.client.create_collection(
            name=collection_name,
            metadata={"description": "Unified transcription segments across all files"}
        )

        # ãƒãƒƒãƒå‡¦ç†ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨ä¿å­˜
        # Gemini batch embedding: æœ€å¤§100ãƒ†ã‚­ã‚¹ãƒˆ/ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        batch_size = 100
        total_batches = (len(texts) + batch_size - 1) // batch_size

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_metadatas = metadatas[i:i + batch_size]
            batch_ids = ids[i:i + batch_size]

            print(f"   Batch {i//batch_size + 1}/{total_batches}: Generating embeddings for {len(batch_texts)} docs...")

            # Gemini Embeddings APIã§ãƒãƒƒãƒãƒ™ã‚¯ãƒˆãƒ«åŒ–
            try:
                result = genai.embed_content(
                    model="models/text-embedding-004",
                    content=batch_texts,  # ãƒªã‚¹ãƒˆå…¨ä½“ã‚’æ¸¡ã™
                    task_type="retrieval_document"
                )

                # çµæœã®å–å¾—: result['embedding'] = [[[emb1]], [[emb2]], ...] ã®å½¢å¼
                # æœ€åˆã®æ¬¡å…ƒã‚’å–ã‚Šé™¤ã: [[[emb]]] -> [[emb]] -> [emb]
                embeddings_data = result['embedding']
                batch_embeddings = [emb[0] if isinstance(emb, list) and isinstance(emb[0], list) else emb
                                   for emb in embeddings_data]

                print(f"      âœ“ Generated {len(batch_embeddings)} embeddings")

            except Exception as e:
                print(f"\n   âš ï¸  Batch embedding failed: {e}")
                print(f"   Falling back to individual calls...")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å€‹åˆ¥ã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–
                batch_embeddings = []
                for j, text in enumerate(batch_texts, 1):
                    try:
                        result = genai.embed_content(
                            model="models/text-embedding-004",
                            content=text,
                            task_type="retrieval_document"
                        )
                        batch_embeddings.append(result['embedding'])
                        if j % 10 == 0:
                            print(f"      Progress: {j}/{len(batch_texts)}", end='\r')
                    except Exception as e2:
                        print(f"\n      Error on doc {j}: {e2}")
                        batch_embeddings.append([0.0] * 768)
                print(f"      Progress: {len(batch_texts)}/{len(batch_texts)} âœ“")

            # ChromaDBã«ä¿å­˜
            collection.add(
                documents=batch_texts,
                embeddings=batch_embeddings,
                metadatas=batch_metadatas,
                ids=batch_ids
            )

            print(f"   âœ… Batch {i//batch_size + 1}/{total_batches} completed")

            # Rate limitå¯¾ç­–ï¼ˆFREE tier: 1500 requests/day = ç´„1.04 req/minï¼‰
            # å®‰å…¨ã®ãŸã‚2ç§’å¾…æ©Ÿ
            if i + batch_size < len(texts):
                time.sleep(2)

        print(f"âœ… Unified vector index built successfully")
        print(f"   Total documents: {collection.count()}")
        print(f"   Collection: {collection_name}")

    def verify_unified_index(self, collection_name: str = "transcripts_unified") -> None:
        """çµ±åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ¤œè¨¼ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªå®Ÿè¡Œï¼‰"""
        print(f"\nğŸ” Verifying unified index...")

        collection = self.client.get_collection(name=collection_name)

        # ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒª
        test_query = "èµ·æ¥­"
        print(f"   Test query: '{test_query}'")

        result = genai.embed_content(
            model="models/text-embedding-004",
            content=test_query,
            task_type="retrieval_query"
        )
        query_embedding = result['embedding']

        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=3
        )

        print(f"\nâœ… Top 3 results from unified index:")
        for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):
            print(f"\n   {i}. {doc[:100]}...")
            print(f"      Source: {metadata.get('source_file', 'N/A')}")
            print(f"      Speaker: {metadata.get('speaker', 'N/A')}")
            print(f"      Timestamp: {metadata.get('timestamp', 'N/A')}")
            print(f"      Topics: {metadata.get('segment_topics', 'N/A')}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    if len(sys.argv) < 2:
        print("Usage: python build_unified_vector_index.py <enhanced_json1> <enhanced_json2> ...")
        print("Example: python build_unified_vector_index.py downloads/*_enhanced.json")
        sys.exit(1)

    json_files = sys.argv[1:]

    print("=" * 70)
    print("Phase 8-3: Unified Vector Index Builder")
    print("=" * 70)

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ“ãƒ«ãƒ€ãƒ¼åˆæœŸåŒ–
    builder = UnifiedVectorIndexBuilder(chroma_path="chroma_db")

    # çµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæº–å‚™
    texts, metadatas, ids = builder.prepare_unified_documents(json_files)

    # çµ±åˆãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
    builder.build_unified_index(texts, metadatas, ids, collection_name="transcripts_unified")

    # æ¤œè¨¼
    builder.verify_unified_index(collection_name="transcripts_unified")

    print("\n" + "=" * 70)
    print("âœ… Unified vector index building completed!")
    print(f"   Total files: {len(json_files)}")
    print(f"   Total documents: {len(texts)}")
    print(f"   Collection: transcripts_unified")
    print("=" * 70)


if __name__ == "__main__":
    main()
