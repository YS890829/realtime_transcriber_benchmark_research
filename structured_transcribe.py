#!/usr/bin/env python3
"""
Structured Transcription with Metadata (Phase 6-1)
ä½¿ã„æ–¹: python structured_transcribe.py <éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹>
æ©Ÿèƒ½: OpenAI Whisper API (word-level timestamps) + Gemini APIè¦ç´„ + JSONæ§‹é€ åŒ–
"""

import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv
import google.generativeai as genai

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# OpenAI Whisper API file size limit (25MB)
MAX_FILE_SIZE = 25 * 1024 * 1024  # 25MB in bytes


def split_audio_file(file_path, chunk_duration=600):
    """
    Split large audio file into chunks using ffmpeg
    """
    file_path = Path(file_path)
    output_dir = file_path.parent / f"{file_path.stem}_chunks"
    output_dir.mkdir(exist_ok=True)

    output_pattern = str(output_dir / f"chunk_%03d{file_path.suffix}")

    cmd = [
        'ffmpeg',
        '-i', str(file_path),
        '-f', 'segment',
        '-segment_time', str(chunk_duration),
        '-c', 'copy',
        output_pattern
    ]

    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode != 0:
        raise Exception(f"ffmpeg failed: {result.stderr}")

    chunks = sorted(output_dir.glob(f"chunk_*{file_path.suffix}"))
    return chunks


def transcribe_audio_with_timestamps(file_path):
    """
    OpenAI Whisper APIã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—ï¼ˆword-level timestampsä»˜ãï¼‰

    æˆ»ã‚Šå€¤:
        dict: {
            "text": å…¨æ–‡,
            "segments": [ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ],
            "words": [å˜èªãƒªã‚¹ãƒˆ] or None
        }
    """
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    file_size = os.path.getsize(file_path)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆ25MBè¶…éã®å ´åˆã¯åˆ†å‰²ï¼‰
    if file_size > MAX_FILE_SIZE:
        print(f"  File size: {file_size / 1024 / 1024:.1f}MB (exceeds 25MB limit)")
        print(f"  Splitting into chunks...")

        chunks = split_audio_file(file_path)
        print(f"  Created {len(chunks)} chunks")

        # å„ãƒãƒ£ãƒ³ã‚¯ã‚’æ–‡å­—èµ·ã“ã—
        all_segments = []
        all_words = []
        full_text_parts = []
        time_offset = 0.0

        for i, chunk_path in enumerate(chunks, 1):
            print(f"  Transcribing chunk {i}/{len(chunks)}...", end='\r')

            with open(chunk_path, "rb") as audio_file:
                response = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="ja",
                    response_format="verbose_json",
                    timestamp_granularities=["word", "segment"]
                )

            # ãƒ†ã‚­ã‚¹ãƒˆè¿½åŠ 
            full_text_parts.append(response.text)

            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè¿½åŠ ï¼ˆæ™‚åˆ»ã‚ªãƒ•ã‚»ãƒƒãƒˆèª¿æ•´ï¼‰
            if hasattr(response, 'segments') and response.segments:
                for seg in response.segments:
                    segment = {
                        "id": len(all_segments) + 1,
                        "start": seg.start + time_offset,
                        "end": seg.end + time_offset,
                        "text": seg.text
                    }
                    all_segments.append(segment)

            # å˜èªè¿½åŠ ï¼ˆæ™‚åˆ»ã‚ªãƒ•ã‚»ãƒƒãƒˆèª¿æ•´ï¼‰
            if hasattr(response, 'words') and response.words:
                for word in response.words:
                    word_data = {
                        "word": word.word,
                        "start": word.start + time_offset,
                        "end": word.end + time_offset
                    }
                    all_words.append(word_data)

            # æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’æ›´æ–°
            if hasattr(response, 'segments') and response.segments and len(response.segments) > 0:
                time_offset = all_segments[-1]['end']

        print()  # æ”¹è¡Œ

        # ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤
        for chunk in chunks:
            chunk.unlink()
        chunks[0].parent.rmdir()

        return {
            "text": "\n\n".join(full_text_parts),
            "segments": all_segments,
            "words": all_words if all_words else None
        }

    else:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒ25MBä»¥ä¸‹ã®å ´åˆã¯é€šå¸¸å‡¦ç†
        with open(file_path, "rb") as audio_file:
            response = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="ja",
                response_format="verbose_json",
                timestamp_granularities=["word", "segment"]
            )

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†
        segments = []
        if hasattr(response, 'segments') and response.segments:
            for i, seg in enumerate(response.segments, 1):
                segment = {
                    "id": i,
                    "start": seg.start,
                    "end": seg.end,
                    "text": seg.text
                }
                segments.append(segment)

        # å˜èªå‡¦ç†
        words = []
        if hasattr(response, 'words') and response.words:
            for word in response.words:
                word_data = {
                    "word": word.word,
                    "start": word.start,
                    "end": word.end
                }
                words.append(word_data)

        return {
            "text": response.text,
            "segments": segments,
            "words": words if words else None
        }


def summarize_text(text):
    """
    Gemini APIã§ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„
    """
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    model = genai.GenerativeModel("gemini-2.0-flash")

    prompt = f"""ä»¥ä¸‹ã®æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ã€è¦ç´„å½¢å¼ã€‘
1. ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ï¼ˆ2-3è¡Œï¼‰
2. ä¸»è¦ãƒã‚¤ãƒ³ãƒˆï¼ˆç®‡æ¡æ›¸ãã€3-5é …ç›®ï¼‰
3. è©³ç´°ã‚µãƒãƒªãƒ¼ï¼ˆæ®µè½å½¢å¼ï¼‰

ã€æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã€‘
{text}
"""

    response = model.generate_content(prompt)
    return response.text


def extract_file_metadata(audio_path):
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    """
    path = Path(audio_path)
    stat = path.stat()

    # ffprobeã§éŸ³å£°é•·ã‚’å–å¾—
    try:
        cmd = [
            'ffprobe',
            '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1',
            str(path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        duration = float(result.stdout.strip()) if result.returncode == 0 else None
    except:
        duration = None

    return {
        "file_name": path.name,
        "file_size_bytes": stat.st_size,
        "format": path.suffix[1:] if path.suffix else None,
        "recorded_at": datetime.fromtimestamp(stat.st_ctime).isoformat(),
        "duration_seconds": duration
    }


def extract_transcription_metadata(text, segments):
    """
    æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    """
    char_count = len(text)
    word_count = len(text.split())
    sentence_count = text.count('ã€‚') + text.count('.') + text.count('!') + text.count('ï¼Ÿ')

    return {
        "transcribed_at": datetime.now().isoformat(),
        "language": "ja",
        "char_count": char_count,
        "word_count": word_count,
        "sentence_count": sentence_count,
        "segment_count": len(segments)
    }


def create_structured_json(audio_path, transcription_result, summary):
    """
    æ§‹é€ åŒ–ã•ã‚ŒãŸJSONã‚’ç”Ÿæˆ
    """
    file_metadata = extract_file_metadata(audio_path)
    transcription_metadata = extract_transcription_metadata(
        transcription_result["text"],
        transcription_result["segments"]
    )

    structured_data = {
        "metadata": {
            "file": file_metadata,
            "transcription": transcription_metadata
        },
        "segments": transcription_result["segments"],
        "words": transcription_result["words"],
        "full_text": transcription_result["text"],
        "summary": summary
    }

    return structured_data


def save_json(data, output_path):
    """
    JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"âœ… JSONä¿å­˜å®Œäº†: {output_path}")


def main():
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãƒã‚§ãƒƒã‚¯
    if len(sys.argv) < 2:
        print("ä½¿ã„æ–¹: python structured_transcribe.py <éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹>")
        sys.exit(1)

    audio_path = sys.argv[1]

    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    if not os.path.exists(audio_path):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {audio_path}")
        sys.exit(1)

    print(f"ğŸ™ï¸ æ§‹é€ åŒ–æ–‡å­—èµ·ã“ã—é–‹å§‹: {audio_path}")
    print("[1/3] æ–‡å­—èµ·ã“ã—ä¸­ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰...")

    # æ–‡å­—èµ·ã“ã—å®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
    transcription_result = transcribe_audio_with_timestamps(audio_path)

    print("[2/3] è¦ç´„ç”Ÿæˆä¸­...")

    # è¦ç´„ç”Ÿæˆ
    summary = summarize_text(transcription_result["text"])

    print("[3/3] JSONæ§‹é€ åŒ–ä¸­...")

    # æ§‹é€ åŒ–JSONç”Ÿæˆ
    structured_data = create_structured_json(audio_path, transcription_result, summary)

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
    base_path = audio_path.rsplit(".", 1)[0]
    json_path = base_path + "_structured.json"

    # JSONä¿å­˜
    save_json(structured_data, json_path)

    # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
    print("\nğŸ“Š å‡¦ç†çµ±è¨ˆ:")
    print(f"  æ–‡å­—æ•°: {structured_data['metadata']['transcription']['char_count']}")
    print(f"  å˜èªæ•°: {structured_data['metadata']['transcription']['word_count']}")
    print(f"  ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {structured_data['metadata']['transcription']['segment_count']}")
    if structured_data['words']:
        print(f"  å˜èªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ•°: {len(structured_data['words'])}")

    if structured_data['metadata']['file']['duration_seconds']:
        duration = structured_data['metadata']['file']['duration_seconds']
        print(f"  éŸ³å£°é•·: {duration:.1f}ç§’ ({duration/60:.1f}åˆ†)")

    print("\nğŸ‰ å®Œäº†!")


if __name__ == "__main__":
    main()
