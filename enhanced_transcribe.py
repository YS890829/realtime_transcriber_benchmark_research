#!/usr/bin/env python3
"""
Phase 6-2: è©±è€…è­˜åˆ¥ã¨ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ + è©±è€…è­˜åˆ¥ + ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º + ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºã‚’å®Ÿè¡Œ
"""

import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI
import google.generativeai as genai

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

def transcribe_audio_with_timestamps(file_path):
    """
    OpenAI Whisper APIã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—ï¼ˆword-level timestampsä»˜ãï¼‰

    Args:
        file_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

    Returns:
        dict: {
            "text": å…¨æ–‡,
            "segments": [ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ],
            "words": [å˜èªãƒªã‚¹ãƒˆ] or None
        }
    """
    print(f"[1/5] æ–‡å­—èµ·ã“ã—ä¸­ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰...")

    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆ25MBåˆ¶é™ï¼‰
    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)

    if file_size_mb <= 25:
        # é€šå¸¸å‡¦ç†
        with open(file_path, "rb") as audio_file:
            response = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="ja",
                response_format="verbose_json",
                timestamp_granularities=["word", "segment"]
            )

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†
        segments = []
        if hasattr(response, 'segments') and response.segments:
            for i, seg in enumerate(response.segments, 1):
                segment = {
                    "id": i,
                    "start": seg.start,
                    "end": seg.end,
                    "text": seg.text
                }
                segments.append(segment)

        # å˜èªå‡¦ç†
        words = []
        if hasattr(response, 'words') and response.words:
            for word in response.words:
                word_data = {
                    "word": word.word,
                    "start": word.start,
                    "end": word.end
                }
                words.append(word_data)

        return {
            "text": response.text,
            "segments": segments,
            "words": words
        }

    else:
        # 25MBè¶…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        print(f"  File size: {file_size_mb:.1f}MB (exceeds 25MB limit)")
        print(f"  Splitting into chunks...")

        # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ï¼ˆæ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨ï¼‰
        return transcribe_large_file_chunked(file_path, client)


def transcribe_large_file_chunked(file_path, client):
    """25MBè¶…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦æ–‡å­—èµ·ã“ã—"""
    chunk_duration = 600  # 10åˆ†ã”ã¨ã«åˆ†å‰²
    chunks_dir = Path("chunks")
    chunks_dir.mkdir(exist_ok=True)

    # ffmpegã§ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
    subprocess.run([
        "ffmpeg", "-i", file_path,
        "-f", "segment",
        "-segment_time", str(chunk_duration),
        "-c", "copy",
        str(chunks_dir / "chunk_%03d.m4a")
    ], check=True, capture_output=True)

    chunk_files = sorted(chunks_dir.glob("chunk_*.m4a"))
    print(f"  Created {len(chunk_files)} chunks")

    all_segments = []
    all_words = []
    full_text_parts = []
    time_offset = 0.0

    for i, chunk_file in enumerate(chunk_files, 1):
        print(f"  Transcribing chunk {i}/{len(chunk_files)}...", end=" ")

        with open(chunk_file, "rb") as audio_file:
            response = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="ja",
                response_format="verbose_json",
                timestamp_granularities=["word", "segment"]
            )

        # ãƒ†ã‚­ã‚¹ãƒˆè¿½åŠ 
        full_text_parts.append(response.text)

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè¿½åŠ ï¼ˆæ™‚åˆ»ã‚ªãƒ•ã‚»ãƒƒãƒˆèª¿æ•´ï¼‰
        if hasattr(response, 'segments') and response.segments:
            for seg in response.segments:
                segment = {
                    "id": len(all_segments) + 1,
                    "start": seg.start + time_offset,
                    "end": seg.end + time_offset,
                    "text": seg.text
                }
                all_segments.append(segment)

        # å˜èªè¿½åŠ ï¼ˆæ™‚åˆ»ã‚ªãƒ•ã‚»ãƒƒãƒˆèª¿æ•´ï¼‰
        if hasattr(response, 'words') and response.words:
            for word in response.words:
                word_data = {
                    "word": word.word,
                    "start": word.start + time_offset,
                    "end": word.end + time_offset
                }
                all_words.append(word_data)

        # æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’æ›´æ–°
        if hasattr(response, 'segments') and response.segments and len(response.segments) > 0:
            time_offset = all_segments[-1]['end']

        # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        chunk_file.unlink()

    # chunksãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤
    chunks_dir.rmdir()

    return {
        "text": " ".join(full_text_parts),
        "segments": all_segments,
        "words": all_words
    }


def perform_speaker_diarization(audio_path, transcription_segments):
    """
    è©±è€…è­˜åˆ¥ï¼ˆSpeaker Diarizationï¼‰
    pyannote.audioã‚’ä½¿ç”¨ã—ã¦ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã”ã¨ã«è©±è€…ãƒ©ãƒ™ãƒ«ã‚’ä»˜ä¸

    Args:
        audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        transcription_segments: æ–‡å­—èµ·ã“ã—ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ

    Returns:
        dict: {
            "speakers": [è©±è€…æƒ…å ±ãƒªã‚¹ãƒˆ],
            "segments_with_speaker": [è©±è€…ãƒ©ãƒ™ãƒ«ä»˜ãã‚»ã‚°ãƒ¡ãƒ³ãƒˆ]
        }
    """
    print(f"[2/5] è©±è€…è­˜åˆ¥ä¸­...")

    try:
        from pyannote.audio import Pipeline

        # HuggingFace tokenãŒå¿…è¦
        hf_token = os.getenv("HUGGINGFACE_TOKEN")
        if not hf_token:
            print("  Warning: HUGGINGFACE_TOKEN not found. Skipping speaker diarization.")
            return fallback_speaker_assignment(transcription_segments)

        # pyannote.audio diarizationãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=hf_token
        )

        # è©±è€…è­˜åˆ¥å®Ÿè¡Œ
        diarization = pipeline(audio_path)

        # è©±è€…ã”ã¨ã®æƒ…å ±ã‚’é›†è¨ˆ
        speakers_info = {}
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            if speaker not in speakers_info:
                speakers_info[speaker] = {
                    "id": speaker,
                    "label": f"Speaker {len(speakers_info) + 1}",
                    "total_duration": 0.0,
                    "segments": []
                }
            speakers_info[speaker]["total_duration"] += turn.duration

        # å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«è©±è€…ãƒ©ãƒ™ãƒ«ã‚’å‰²ã‚Šå½“ã¦
        segments_with_speaker = []
        for seg in transcription_segments:
            seg_start = seg["start"]
            seg_end = seg["end"]

            # ã“ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¨æœ€ã‚‚é‡è¤‡ã™ã‚‹è©±è€…ã‚’æ¢ã™
            max_overlap = 0.0
            assigned_speaker = "SPEAKER_00"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

            for turn, _, speaker in diarization.itertracks(yield_label=True):
                overlap_start = max(seg_start, turn.start)
                overlap_end = min(seg_end, turn.end)
                overlap = max(0, overlap_end - overlap_start)

                if overlap > max_overlap:
                    max_overlap = overlap
                    assigned_speaker = speaker

            seg_copy = seg.copy()
            seg_copy["speaker"] = assigned_speaker
            segments_with_speaker.append(seg_copy)

        # è©±è€…ãƒªã‚¹ãƒˆä½œæˆ
        speakers_list = []
        total_duration = sum(s["total_duration"] for s in speakers_info.values())

        for speaker_id, info in speakers_info.items():
            speakers_list.append({
                "id": speaker_id,
                "label": info["label"],
                "total_duration": round(info["total_duration"], 2),
                "speaking_percentage": round((info["total_duration"] / total_duration) * 100, 1) if total_duration > 0 else 0
            })

        print(f"  Detected {len(speakers_list)} speakers")

        return {
            "speakers": speakers_list,
            "segments_with_speaker": segments_with_speaker
        }

    except Exception as e:
        print(f"  Error in speaker diarization: {e}")
        print(f"  Falling back to single speaker assumption")
        return fallback_speaker_assignment(transcription_segments)


def fallback_speaker_assignment(transcription_segments):
    """è©±è€…è­˜åˆ¥å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å˜ä¸€è©±è€…ã¨ã—ã¦æ‰±ã†"""
    total_duration = sum(seg["end"] - seg["start"] for seg in transcription_segments)

    segments_with_speaker = []
    for seg in transcription_segments:
        seg_copy = seg.copy()
        seg_copy["speaker"] = "SPEAKER_00"
        segments_with_speaker.append(seg_copy)

    return {
        "speakers": [{
            "id": "SPEAKER_00",
            "label": "Speaker 1",
            "total_duration": round(total_duration, 2),
            "speaking_percentage": 100.0
        }],
        "segments_with_speaker": segments_with_speaker
    }


def extract_topics_and_entities(full_text, segments_with_speaker):
    """
    Gemini APIã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºã¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º

    Args:
        full_text: å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆ
        segments_with_speaker: è©±è€…ãƒ©ãƒ™ãƒ«ä»˜ãã‚»ã‚°ãƒ¡ãƒ³ãƒˆ

    Returns:
        dict: {
            "topics": [ãƒˆãƒ”ãƒƒã‚¯ãƒªã‚¹ãƒˆ],
            "entities": {people, organizations, dates, action_items},
            "segments_enhanced": [ãƒˆãƒ”ãƒƒã‚¯ãƒ©ãƒ™ãƒ«ä»˜ãã‚»ã‚°ãƒ¡ãƒ³ãƒˆ]
        }
    """
    print(f"[3/5] ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºä¸­...")

    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    model = genai.GenerativeModel("gemini-2.0-flash-exp")

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    prompt = f"""
ä»¥ä¸‹ã®æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

{{
  "topics": [
    {{
      "id": "topic_1",
      "name": "ãƒˆãƒ”ãƒƒã‚¯å",
      "summary": "ãƒˆãƒ”ãƒƒã‚¯ã®è¦ç´„ï¼ˆ1-2æ–‡ï¼‰",
      "keywords": ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2"]
    }}
  ],
  "entities": {{
    "people": ["äººå1", "äººå2"],
    "organizations": ["çµ„ç¹”å1", "çµ„ç¹”å2"],
    "dates": ["æ—¥ä»˜è¡¨ç¾1", "æ—¥ä»˜è¡¨ç¾2"],
    "action_items": ["ã‚¢ã‚¯ã‚·ãƒ§ãƒ³1", "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³2"]
  }}
}}

æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆ:
{full_text}
"""

    try:
        response = model.generate_content(prompt)
        response_text = response.text.strip()

        # JSONã‚’æŠ½å‡ºï¼ˆ```jsonãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»ï¼‰
        if "```json" in response_text:
            response_text = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            response_text = response_text.split("```")[1].split("```")[0].strip()

        result = json.loads(response_text)

        print(f"  Extracted {len(result.get('topics', []))} topics")
        print(f"  Found {len(result.get('entities', {}).get('people', []))} people")

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ãƒˆãƒ”ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦ï¼ˆç°¡æ˜“å®Ÿè£…ï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ï¼‰
        segments_enhanced = assign_topics_to_segments(
            segments_with_speaker,
            result.get("topics", [])
        )

        return {
            "topics": result.get("topics", []),
            "entities": result.get("entities", {}),
            "segments_enhanced": segments_enhanced
        }

    except Exception as e:
        print(f"  Error in topic extraction: {e}")
        return {
            "topics": [],
            "entities": {"people": [], "organizations": [], "dates": [], "action_items": []},
            "segments_enhanced": segments_with_speaker
        }


def assign_topics_to_segments(segments, topics):
    """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ãƒˆãƒ”ãƒƒã‚¯ãƒ©ãƒ™ãƒ«ã‚’å‰²ã‚Šå½“ã¦ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ï¼‰"""
    segments_enhanced = []

    for seg in segments:
        seg_text = seg["text"]
        assigned_topics = []

        # å„ãƒˆãƒ”ãƒƒã‚¯ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«å«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for topic in topics:
            keywords = topic.get("keywords", [])
            if any(keyword in seg_text for keyword in keywords):
                assigned_topics.append(topic["id"])

        seg_copy = seg.copy()
        seg_copy["topics"] = assigned_topics if assigned_topics else []
        seg_copy["keywords"] = []  # å¾Œã§å®Ÿè£…
        segments_enhanced.append(seg_copy)

    return segments_enhanced


def generate_enhanced_summary(full_text, topics, entities, speakers):
    """æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ãŸé«˜ç²¾åº¦è¦ç´„ç”Ÿæˆ"""
    print(f"[4/5] è¦ç´„ç”Ÿæˆä¸­ï¼ˆæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æ´»ç”¨ï¼‰...")

    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    model = genai.GenerativeModel("gemini-2.0-flash-exp")

    # è©±è€…æƒ…å ±ã‚’æ–‡å­—åˆ—åŒ–
    speakers_info = "\n".join([
        f"- {s['label']}: {s['speaking_percentage']}% ({s['total_duration']}ç§’)"
        for s in speakers
    ])

    # ãƒˆãƒ”ãƒƒã‚¯æƒ…å ±ã‚’æ–‡å­—åˆ—åŒ–
    topics_info = "\n".join([
        f"- {t['name']}: {t['summary']}"
        for t in topics
    ])

    # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æƒ…å ±ã‚’æ–‡å­—åˆ—åŒ–
    entities_info = f"""
- äººç‰©: {', '.join(entities.get('people', []))}
- çµ„ç¹”: {', '.join(entities.get('organizations', []))}
- æ—¥ä»˜: {', '.join(entities.get('dates', []))}
- ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {', '.join(entities.get('action_items', []))}
"""

    prompt = f"""
ä»¥ä¸‹ã®æƒ…å ±ã‚’å…ƒã«ã€ä¼šè©±ã®è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€è©±è€…æƒ…å ±ã€‘
{speakers_info}

ã€ãƒˆãƒ”ãƒƒã‚¯ã€‘
{topics_info}

ã€æŠ½å‡ºã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã€‘
{entities_info}

ã€å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã€‘
{full_text[:5000]}...  # æœ€åˆã®5000æ–‡å­—ã®ã¿

ä»¥ä¸‹ã®å½¢å¼ã§è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

# ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼
ï¼ˆ2-3æ–‡ã§å…¨ä½“ã®æ¦‚è¦ï¼‰

# ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ
- ãƒã‚¤ãƒ³ãƒˆ1
- ãƒã‚¤ãƒ³ãƒˆ2
- ãƒã‚¤ãƒ³ãƒˆ3

# è©³ç´°ã‚µãƒãƒªãƒ¼
ï¼ˆå„ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦è©³ç´°ã«èª¬æ˜ï¼‰
"""

    try:
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        print(f"  Error in summary generation: {e}")
        return "è¦ç´„ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ"


def get_file_metadata(audio_path):
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    audio_path = Path(audio_path)

    # ãƒ•ã‚¡ã‚¤ãƒ«åŸºæœ¬æƒ…å ±
    file_size = audio_path.stat().st_size
    file_mtime = datetime.fromtimestamp(audio_path.stat().st_mtime)

    # ffprobeã§éŸ³å£°é•·ã‚’å–å¾—
    try:
        result = subprocess.run(
            ["ffprobe", "-v", "error", "-show_entries",
             "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", str(audio_path)],
            capture_output=True,
            text=True,
            check=True
        )
        duration = float(result.stdout.strip())
    except:
        duration = 0.0

    return {
        "file_name": audio_path.name,
        "file_size_bytes": file_size,
        "format": audio_path.suffix.lstrip('.'),
        "recorded_at": file_mtime.isoformat(),
        "duration_seconds": duration
    }


def create_enhanced_json(audio_path, transcription_result, diarization_result, topics_result, summary):
    """Phase 6-2ã®æ‹¡å¼µJSONæ§‹é€ ã‚’ç”Ÿæˆ"""
    print(f"[5/5] JSONæ§‹é€ åŒ–ä¸­...")

    file_metadata = get_file_metadata(audio_path)

    # æ–‡å­—èµ·ã“ã—ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    full_text = transcription_result["text"]
    segments = topics_result["segments_enhanced"]
    words = transcription_result["words"]

    transcription_metadata = {
        "transcribed_at": datetime.now().isoformat(),
        "language": "ja",
        "char_count": len(full_text),
        "word_count": len(full_text.split()),
        "sentence_count": full_text.count("ã€‚") + full_text.count("ï¼Ÿ") + full_text.count("ï¼"),
        "segment_count": len(segments)
    }

    # çµ±åˆJSONä½œæˆ
    enhanced_data = {
        "metadata": {
            "file": file_metadata,
            "transcription": transcription_metadata
        },
        "speakers": diarization_result["speakers"],
        "topics": topics_result["topics"],
        "entities": topics_result["entities"],
        "segments": segments,
        "words": words,
        "full_text": full_text,
        "summary": summary
    }

    return enhanced_data


def main():
    if len(sys.argv) < 2:
        print("Usage: python enhanced_transcribe.py <audio_file>")
        sys.exit(1)

    audio_path = sys.argv[1]

    if not os.path.exists(audio_path):
        print(f"Error: File not found: {audio_path}")
        sys.exit(1)

    print(f"ğŸ™ï¸ æ‹¡å¼µæ–‡å­—èµ·ã“ã—é–‹å§‹: {audio_path}")

    # Phase 6-1: æ–‡å­—èµ·ã“ã—
    transcription_result = transcribe_audio_with_timestamps(audio_path)

    # Phase 6-2: è©±è€…è­˜åˆ¥
    diarization_result = perform_speaker_diarization(audio_path, transcription_result["segments"])

    # Phase 6-2: ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
    topics_result = extract_topics_and_entities(
        transcription_result["text"],
        diarization_result["segments_with_speaker"]
    )

    # Phase 6-2: æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ãŸè¦ç´„
    summary = generate_enhanced_summary(
        transcription_result["text"],
        topics_result["topics"],
        topics_result["entities"],
        diarization_result["speakers"]
    )

    # Phase 6-2: æ‹¡å¼µJSONä½œæˆ
    enhanced_data = create_enhanced_json(
        audio_path,
        transcription_result,
        diarization_result,
        topics_result,
        summary
    )

    # å‡ºåŠ›
    output_path = Path(audio_path).with_suffix('').name + "_enhanced.json"
    output_dir = Path(audio_path).parent
    output_file = output_dir / output_path

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(enhanced_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… JSONä¿å­˜å®Œäº†: {output_file}")

    # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
    print(f"\nğŸ“Š å‡¦ç†çµ±è¨ˆ:")
    print(f"  æ–‡å­—æ•°: {enhanced_data['metadata']['transcription']['char_count']}")
    print(f"  å˜èªæ•°: {enhanced_data['metadata']['transcription']['word_count']}")
    print(f"  ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {enhanced_data['metadata']['transcription']['segment_count']}")
    print(f"  è©±è€…æ•°: {len(enhanced_data['speakers'])}")
    print(f"  ãƒˆãƒ”ãƒƒã‚¯æ•°: {len(enhanced_data['topics'])}")
    print(f"  å˜èªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ•°: {len(enhanced_data['words']) if enhanced_data['words'] else 0}")
    print(f"  éŸ³å£°é•·: {enhanced_data['metadata']['file']['duration_seconds']:.1f}ç§’ ({enhanced_data['metadata']['file']['duration_seconds']/60:.1f}åˆ†)")

    print(f"\nğŸ‰ å®Œäº†!")


if __name__ == "__main__":
    main()
