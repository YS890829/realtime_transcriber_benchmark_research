#!/usr/bin/env python3
"""
Phase 6-3 Stage 3-1: Cross-Meeting Analysis
è¤‡æ•°ã®æ–‡å­—èµ·ã“ã—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¨ªæ–­ã—ã¦åˆ†æ

æ©Ÿèƒ½:
- è¤‡æ•°ã®enhanced JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
- å…±é€šãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºï¼ˆè¤‡æ•°ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã§ç¹°ã‚Šè¿”ã•ã‚Œã‚‹ãƒ†ãƒ¼ãƒï¼‰
- ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£è¿½è·¡ï¼ˆäººç‰©ãƒ»çµ„ç¹”ãŒè¤‡æ•°ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã§ã©ã†è¨€åŠã•ã‚Œã‚‹ã‹ï¼‰
- ãƒˆãƒ”ãƒƒã‚¯ã®æ™‚ç³»åˆ—å¤‰åŒ–åˆ†æ
- ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã®é€²æ—è¿½è·¡
- çµ±åˆã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ
"""

import json
import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Tuple
from datetime import datetime
from collections import Counter, defaultdict
from dotenv import load_dotenv
import google.generativeai as genai

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# Gemini APIã‚­ãƒ¼é¸æŠï¼ˆFREE/PAID tierï¼‰
use_paid_tier = os.getenv("USE_PAID_TIER", "").lower() == "true"
api_key = os.getenv("GEMINI_API_KEY_PAID") if use_paid_tier else os.getenv("GEMINI_API_KEY")

if not api_key:
    print("âŒ Error: GEMINI_API_KEY not found in environment variables")
    sys.exit(1)

genai.configure(api_key=api_key)
print(f"âœ… Using Gemini API: {'PAID' if use_paid_tier else 'FREE'} tier")


class CrossMeetingAnalyzer:
    """è¤‡æ•°ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ¨ªæ–­åˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.llm = genai.GenerativeModel("gemini-2.0-flash-exp")

        print("âœ… Cross-Meeting Analyzer initialized")
        print("   LLM: gemini-2.0-flash-exp")

    def load_transcripts(self, json_paths: List[str]) -> List[Dict[str, Any]]:
        """
        è¤‡æ•°ã®enhanced JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿

        Args:
            json_paths: JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ

        Returns:
            èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        print(f"\nğŸ“‚ Loading {len(json_paths)} transcript files...")

        transcripts = []
        for path in json_paths:
            if not os.path.exists(path):
                print(f"   âš ï¸  File not found: {path}")
                continue

            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # ãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            data['_file_path'] = path
            data['_file_name'] = Path(path).name

            transcripts.append(data)

            # ç°¡å˜ãªçµ±è¨ˆã‚’è¡¨ç¤º
            file_name = data.get('metadata', {}).get('file', {}).get('file_name', Path(path).name)
            num_segments = len(data.get('segments', []))
            num_topics = len(data.get('topics', []))
            print(f"   âœ… {file_name}: {num_segments} segments, {num_topics} topics")

        print(f"\nâœ… Loaded {len(transcripts)} transcripts")
        return transcripts

    def extract_common_topics(self, transcripts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        å…±é€šãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º

        Args:
            transcripts: æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ

        Returns:
            å…±é€šãƒˆãƒ”ãƒƒã‚¯åˆ†æçµæœ
        """
        print(f"\nğŸ” Extracting common topics across {len(transcripts)} meetings...")

        # ã™ã¹ã¦ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’åé›†
        all_topics = []
        topic_by_file = {}

        for transcript in transcripts:
            file_name = transcript.get('_file_name', 'Unknown')
            topics = transcript.get('topics', [])

            topic_by_file[file_name] = [t.get('name', '') for t in topics]
            all_topics.extend(topics)

        # ãƒˆãƒ”ãƒƒã‚¯åã®å‡ºç¾é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        topic_names = [t.get('name', '') for t in all_topics if t.get('name')]
        topic_counts = Counter(topic_names)

        # å…±é€šãƒˆãƒ”ãƒƒã‚¯ï¼ˆ2å›ä»¥ä¸Šå‡ºç¾ï¼‰
        common_topics = {name: count for name, count in topic_counts.items() if count >= 2}

        print(f"   Total topics: {len(all_topics)}")
        print(f"   Unique topics: {len(topic_counts)}")
        print(f"   Common topics (appearing 2+ times): {len(common_topics)}")

        if common_topics:
            print(f"\n   Top common topics:")
            for name, count in sorted(common_topics.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"   - {name}: {count} occurrences")

        return {
            "all_topics": all_topics,
            "topic_counts": dict(topic_counts),
            "common_topics": common_topics,
            "topics_by_file": topic_by_file
        }

    def track_entities(self, transcripts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£è¿½è·¡ï¼ˆäººç‰©ãƒ»çµ„ç¹”ãƒ»æ—¥ä»˜ï¼‰

        Args:
            transcripts: æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ

        Returns:
            ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£è¿½è·¡çµæœ
        """
        print(f"\nğŸ‘¥ Tracking entities across {len(transcripts)} meetings...")

        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’åé›†
        people_by_file = {}
        organizations_by_file = {}
        dates_by_file = {}

        all_people = []
        all_organizations = []
        all_dates = []

        for transcript in transcripts:
            file_name = transcript.get('_file_name', 'Unknown')
            entities = transcript.get('entities', {})

            people = entities.get('people', [])
            orgs = entities.get('organizations', [])
            dates = entities.get('dates', [])

            people_by_file[file_name] = people
            organizations_by_file[file_name] = orgs
            dates_by_file[file_name] = dates

            all_people.extend(people)
            all_organizations.extend(orgs)
            all_dates.extend(dates)

        # å‡ºç¾é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        people_counts = Counter(all_people)
        org_counts = Counter(all_organizations)
        date_counts = Counter(all_dates)

        # è¤‡æ•°å›å‡ºç¾ã™ã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
        recurring_people = {name: count for name, count in people_counts.items() if count >= 2}
        recurring_orgs = {name: count for name, count in org_counts.items() if count >= 2}

        print(f"   Total people mentioned: {len(people_counts)}")
        print(f"   Recurring people (2+ meetings): {len(recurring_people)}")
        print(f"   Total organizations: {len(org_counts)}")
        print(f"   Recurring organizations (2+ meetings): {len(recurring_orgs)}")

        if recurring_people:
            print(f"\n   Recurring people:")
            for name, count in sorted(recurring_people.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"   - {name}: {count} occurrences")

        if recurring_orgs:
            print(f"\n   Recurring organizations:")
            for name, count in sorted(recurring_orgs.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"   - {name}: {count} occurrences")

        return {
            "people_counts": dict(people_counts),
            "org_counts": dict(org_counts),
            "date_counts": dict(date_counts),
            "recurring_people": recurring_people,
            "recurring_orgs": recurring_orgs,
            "people_by_file": people_by_file,
            "organizations_by_file": organizations_by_file,
            "dates_by_file": dates_by_file
        }

    def analyze_action_items(self, transcripts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã‚’æŠ½å‡ºã—ã¦åˆ†æ

        Args:
            transcripts: æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ

        Returns:
            ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ åˆ†æçµæœ
        """
        print(f"\nğŸ“‹ Analyzing action items across {len(transcripts)} meetings...")

        action_items_by_file = {}
        all_action_items = []

        for transcript in transcripts:
            file_name = transcript.get('_file_name', 'Unknown')
            entities = transcript.get('entities', {})
            action_items = entities.get('action_items', [])

            action_items_by_file[file_name] = action_items
            all_action_items.extend(action_items)

        print(f"   Total action items: {len(all_action_items)}")
        print(f"   Files with action items: {len([f for f, items in action_items_by_file.items() if items])}")

        return {
            "all_action_items": all_action_items,
            "action_items_by_file": action_items_by_file
        }

    def generate_integrated_summary(
        self,
        transcripts: List[Dict[str, Any]],
        topic_analysis: Dict[str, Any],
        entity_analysis: Dict[str, Any],
        action_analysis: Dict[str, Any]
    ) -> str:
        """
        Gemini APIã‚’ä½¿ç”¨ã—ã¦çµ±åˆã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ

        Args:
            transcripts: æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            topic_analysis: ãƒˆãƒ”ãƒƒã‚¯åˆ†æçµæœ
            entity_analysis: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åˆ†æçµæœ
            action_analysis: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ åˆ†æçµæœ

        Returns:
            çµ±åˆã‚µãƒãƒªãƒ¼
        """
        print(f"\nğŸ¤– Generating integrated summary with Gemini...")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        file_summaries = []
        for transcript in transcripts:
            file_name = transcript.get('_file_name', 'Unknown')
            summary = transcript.get('summary', 'è¦ç´„ãªã—')
            topics = [t.get('name', '') for t in transcript.get('topics', [])]

            file_summaries.append(f"""
## {file_name}
ãƒˆãƒ”ãƒƒã‚¯: {', '.join(topics)}
è¦ç´„: {summary}
""")

        prompt = f"""ä»¥ä¸‹ã¯è¤‡æ•°ã®ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚ã“ã‚Œã‚‰ã‚’çµ±åˆã—ã¦åˆ†æã—ã€ç·åˆçš„ãªã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ•°ã€‘
{len(transcripts)}ä»¶

ã€å„ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®è¦ç´„ã€‘
{''.join(file_summaries)}

ã€å…±é€šãƒˆãƒ”ãƒƒã‚¯ã€‘
{', '.join(list(topic_analysis.get('common_topics', {}).keys())[:10])}

ã€ç¹°ã‚Šè¿”ã—ç™»å ´ã™ã‚‹äººç‰©ã€‘
{', '.join(list(entity_analysis.get('recurring_people', {}).keys())[:10])}

ã€ç¹°ã‚Šè¿”ã—ç™»å ´ã™ã‚‹çµ„ç¹”ã€‘
{', '.join(list(entity_analysis.get('recurring_orgs', {}).keys())[:10])}

ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ç·æ•°ã€‘
{len(action_analysis.get('all_action_items', []))}ä»¶

ã€å‡ºåŠ›å½¢å¼ã€‘
ä»¥ä¸‹ã®å½¢å¼ã§Markdownã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

# çµ±åˆã‚µãƒãƒªãƒ¼

## 1. å…¨ä½“æ¦‚è¦
è¤‡æ•°ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°å…¨ä½“ã‚’é€šã˜ãŸä¸»è¦ãªãƒ†ãƒ¼ãƒã¨ç›®çš„

## 2. å…±é€šãƒˆãƒ”ãƒƒã‚¯ã®åˆ†æ
è¤‡æ•°å›ç™»å ´ã™ã‚‹é‡è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¨ãã®å¤‰é·

## 3. ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚½ãƒ³ã¨çµ„ç¹”
ç¹°ã‚Šè¿”ã—ç™»å ´ã™ã‚‹äººç‰©ãƒ»çµ„ç¹”ã¨ãã®å½¹å‰²

## 4. æ™‚ç³»åˆ—ã§ã®å¤‰åŒ–
ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’é€šã˜ãŸè­°è«–ã®é€²å±•ã‚„å¤‰åŒ–

## 5. ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã®ã¾ã¨ã‚
ä»Šå¾Œå–ã‚‹ã¹ãã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®çµ±åˆãƒªã‚¹ãƒˆ

## 6. çµè«–ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
å…¨ä½“ã‚’é€šã˜ãŸçµè«–ã¨æ¨å¥¨ã•ã‚Œã‚‹æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
"""

        response = self.llm.generate_content(prompt)
        summary = response.text.strip()

        print(f"   Summary generated ({len(summary)} characters)")

        return summary

    def analyze(self, json_paths: List[str]) -> Dict[str, Any]:
        """
        ãƒ¡ã‚¤ãƒ³åˆ†æå‡¦ç†

        Args:
            json_paths: JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ

        Returns:
            åˆ†æçµæœ
        """
        print("=" * 70)
        print("Phase 6-3 Stage 3-1: Cross-Meeting Analysis")
        print("=" * 70)

        # 1. ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        transcripts = self.load_transcripts(json_paths)

        if len(transcripts) < 2:
            print("\nâš ï¸  Warning: At least 2 transcripts are recommended for cross-meeting analysis")
            if len(transcripts) == 0:
                print("âŒ No valid transcripts found. Exiting.")
                return {}

        # 2. ãƒˆãƒ”ãƒƒã‚¯åˆ†æ
        topic_analysis = self.extract_common_topics(transcripts)

        # 3. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£è¿½è·¡
        entity_analysis = self.track_entities(transcripts)

        # 4. ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ åˆ†æ
        action_analysis = self.analyze_action_items(transcripts)

        # 5. çµ±åˆã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        integrated_summary = self.generate_integrated_summary(
            transcripts,
            topic_analysis,
            entity_analysis,
            action_analysis
        )

        # çµæœã‚’ã¾ã¨ã‚ã‚‹
        result = {
            "num_transcripts": len(transcripts),
            "transcripts": transcripts,
            "topic_analysis": topic_analysis,
            "entity_analysis": entity_analysis,
            "action_analysis": action_analysis,
            "integrated_summary": integrated_summary
        }

        return result

    def save_report(self, result: Dict[str, Any], output_path: str) -> None:
        """
        åˆ†æçµæœã‚’Markdownãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦ä¿å­˜

        Args:
            result: åˆ†æçµæœ
            output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        print(f"\nğŸ’¾ Saving analysis report to: {output_path}")

        with open(output_path, 'w', encoding='utf-8') as f:
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            f.write("# Cross-Meeting Analysis Report\n\n")
            f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**Number of Meetings:** {result['num_transcripts']}\n\n")

            # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
            f.write("## Analyzed Files\n\n")
            for i, transcript in enumerate(result['transcripts'], 1):
                file_name = transcript.get('_file_name', 'Unknown')
                num_segments = len(transcript.get('segments', []))
                num_topics = len(transcript.get('topics', []))
                f.write(f"{i}. `{file_name}` - {num_segments} segments, {num_topics} topics\n")

            f.write("\n---\n\n")

            # çµ±åˆã‚µãƒãƒªãƒ¼
            f.write(result['integrated_summary'])

            f.write("\n\n---\n\n")

            # çµ±è¨ˆæƒ…å ±
            f.write("## Detailed Statistics\n\n")

            # ãƒˆãƒ”ãƒƒã‚¯çµ±è¨ˆ
            f.write("### Topics\n\n")
            topic_counts = result['topic_analysis']['topic_counts']
            f.write(f"- Total topics: {len(topic_counts)}\n")
            f.write(f"- Common topics (2+ occurrences): {len(result['topic_analysis']['common_topics'])}\n\n")

            if topic_counts:
                f.write("**Top 10 Topics:**\n\n")
                for name, count in sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
                    f.write(f"- {name}: {count} occurrences\n")

            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£çµ±è¨ˆ
            f.write("\n### Entities\n\n")
            people_counts = result['entity_analysis']['people_counts']
            org_counts = result['entity_analysis']['org_counts']

            f.write(f"- Total people: {len(people_counts)}\n")
            f.write(f"- Recurring people: {len(result['entity_analysis']['recurring_people'])}\n")
            f.write(f"- Total organizations: {len(org_counts)}\n")
            f.write(f"- Recurring organizations: {len(result['entity_analysis']['recurring_orgs'])}\n\n")

            if people_counts:
                f.write("**Top 10 People:**\n\n")
                for name, count in sorted(people_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
                    f.write(f"- {name}: {count} occurrences\n")

            if org_counts:
                f.write("\n**Top 10 Organizations:**\n\n")
                for name, count in sorted(org_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
                    f.write(f"- {name}: {count} occurrences\n")

            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ 
            f.write("\n### Action Items\n\n")
            all_action_items = result['action_analysis']['all_action_items']
            f.write(f"- Total action items: {len(all_action_items)}\n\n")

            if all_action_items:
                f.write("**All Action Items:**\n\n")
                for i, item in enumerate(all_action_items, 1):
                    f.write(f"{i}. {item}\n")

        print(f"âœ… Report saved: {output_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    if len(sys.argv) < 2:
        print("Usage: python cross_analysis.py <json_file1> <json_file2> [json_file3 ...]")
        print("Example: python cross_analysis.py 'downloads/*_structured_enhanced.json'")
        sys.exit(1)

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
    json_paths = sys.argv[1:]

    # ã‚°ãƒ­ãƒ–ãƒ‘ã‚¿ãƒ¼ãƒ³å±•é–‹ï¼ˆãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰å¯¾å¿œï¼‰
    expanded_paths = []
    for pattern in json_paths:
        from glob import glob
        matches = glob(pattern)
        if matches:
            expanded_paths.extend(matches)
        else:
            expanded_paths.append(pattern)  # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã—ãªã„å ´åˆã¯ãã®ã¾ã¾è¿½åŠ 

    # é‡è¤‡å‰Šé™¤
    json_paths = list(set(expanded_paths))

    # åˆ†æå®Ÿè¡Œ
    analyzer = CrossMeetingAnalyzer()
    result = analyzer.analyze(json_paths)

    if not result:
        sys.exit(1)

    # çµæœè¡¨ç¤º
    print("\n" + "=" * 70)
    print("Analysis Complete!")
    print("=" * 70)
    print(f"\nğŸ“Š Summary Statistics:")
    print(f"   Meetings analyzed: {result['num_transcripts']}")
    print(f"   Total topics: {len(result['topic_analysis']['topic_counts'])}")
    print(f"   Common topics: {len(result['topic_analysis']['common_topics'])}")
    print(f"   Recurring people: {len(result['entity_analysis']['recurring_people'])}")
    print(f"   Recurring organizations: {len(result['entity_analysis']['recurring_orgs'])}")
    print(f"   Total action items: {len(result['action_analysis']['all_action_items'])}")

    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    output_path = "cross_meeting_analysis_report.md"
    analyzer.save_report(result, output_path)

    print("\n" + "=" * 70)
    print(f"âœ… Cross-meeting analysis completed!")
    print(f"   Report: {output_path}")
    print("=" * 70)


if __name__ == "__main__":
    main()
