#!/usr/bin/env python3
"""
Phase 8-4 Comprehensive Test: ç·åˆæ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è©±è€…æ¨è«–ç²¾åº¦ã€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£çµ±ä¸€ç‡ã€æ¨ªæ–­æ¤œç´¢ã®å‹•ä½œç¢ºèª
"""

import os
import sys
import json
from pathlib import Path
from collections import Counter
from dotenv import load_dotenv
import google.generativeai as genai
from semantic_search import SemanticSearchEngine
from rag_qa import RAGQASystem

load_dotenv()

# Gemini APIè¨­å®š
use_paid_tier = os.getenv("USE_PAID_TIER", "").lower() == "true"
api_key = os.getenv("GEMINI_API_KEY_PAID") if use_paid_tier else os.getenv("GEMINI_API_KEY")

if not api_key:
    print("âŒ Error: GEMINI_API_KEY not found in environment variables")
    sys.exit(1)

genai.configure(api_key=api_key)


def test_speaker_inference():
    """Test 1: è©±è€…æ¨è«–ç²¾åº¦ç¢ºèª"""
    print("\n" + "=" * 70)
    print("Test 1: Speaker Inference Accuracy")
    print("=" * 70)

    enhanced_files = list(Path("downloads").glob("*_structured_enhanced.json"))

    total_segments = 0
    speaker_counts = Counter()

    for enhanced_file in enhanced_files:
        with open(enhanced_file, "r", encoding="utf-8") as f:
            data = json.load(f)

        segments = data.get("segments", [])
        total_segments += len(segments)

        for seg in segments:
            speaker = seg.get("speaker", "Unknown")
            speaker_counts[speaker] += 1

    print(f"\nğŸ“Š Speaker Distribution:")
    print(f"   Total segments: {total_segments}")
    for speaker, count in speaker_counts.most_common():
        percentage = (count / total_segments) * 100
        print(f"   - {speaker}: {count} segments ({percentage:.1f}%)")

    # æˆåŠŸåˆ¤å®š: è©±è€…ãŒç‰¹å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨
    has_speakers = len(speaker_counts) > 0

    if has_speakers:
        print(f"\nâœ… Test 1 PASSED: Speakers identified in {len(enhanced_files)} files")
        return True
    else:
        print("\nâŒ Test 1 FAILED: No speakers found")
        return False


def test_entity_unification():
    """Test 2: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£çµ±ä¸€ç‡ç¢ºèª"""
    print("\n" + "=" * 70)
    print("Test 2: Entity Unification Rate")
    print("=" * 70)

    enhanced_files = list(Path("downloads").glob("*_structured_enhanced.json"))

    all_people = []
    all_organizations = []
    canonical_people = set()
    canonical_orgs = set()
    entity_groups = []

    for enhanced_file in enhanced_files:
        with open(enhanced_file, "r", encoding="utf-8") as f:
            data = json.load(f)

        entities = data.get("entities", {})

        # äººç‰©ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
        people = entities.get("people", [])
        for person in people:
            if isinstance(person, dict):
                all_people.append(person.get("name", ""))
                canonical = person.get("canonical_name", "")
                entity_id = person.get("entity_id", "")
                variants = person.get("variants", [])

                if canonical:
                    canonical_people.add(canonical)

                if len(variants) > 1:
                    entity_groups.append({
                        "type": "person",
                        "canonical": canonical,
                        "variants": variants,
                        "entity_id": entity_id
                    })

        # çµ„ç¹”ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
        organizations = entities.get("organizations", [])
        for org in organizations:
            if isinstance(org, dict):
                all_organizations.append(org.get("name", ""))
                canonical = org.get("canonical_name", "")
                entity_id = org.get("entity_id", "")
                variants = org.get("variants", [])

                if canonical:
                    canonical_orgs.add(canonical)

                if len(variants) > 1:
                    entity_groups.append({
                        "type": "organization",
                        "canonical": canonical,
                        "variants": variants,
                        "entity_id": entity_id
                    })

    print(f"\nğŸ“Š Entity Statistics:")
    print(f"   Total people mentions: {len(all_people)}")
    print(f"   Unique canonical people: {len(canonical_people)}")
    print(f"   Total organization mentions: {len(all_organizations)}")
    print(f"   Unique canonical organizations: {len(canonical_orgs)}")

    print(f"\nğŸ”— Entity Groups (variants unified):")
    if entity_groups:
        for group in entity_groups[:5]:  # Show first 5
            print(f"   [{group['type']}] {group['canonical']} ({group['entity_id']})")
            print(f"      Variants: {', '.join(group['variants'])}")
    else:
        print("   No entity groups found (all entities are unique)")

    # æˆåŠŸåˆ¤å®š: canonical_nameãŒä»˜ä¸ã•ã‚Œã¦ã„ã‚‹ã“ã¨
    has_canonical = len(canonical_people) > 0 or len(canonical_orgs) > 0

    if has_canonical:
        print(f"\nâœ… Test 2 PASSED: Entity unification complete")
        return True
    else:
        print("\nâŒ Test 2 FAILED: No canonical entities found")
        return False


def test_cross_file_search():
    """Test 3: æ¨ªæ–­æ¤œç´¢ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 70)
    print("Test 3: Cross-File Search Verification")
    print("=" * 70)

    engine = SemanticSearchEngine(chroma_path="chroma_db")

    # Test query
    query = "å–¶æ¥­ã‚„ãƒ“ã‚¸ãƒã‚¹æˆ¦ç•¥ã«ã¤ã„ã¦è©±ã—ã¦ã„ã‚‹éƒ¨åˆ†"
    print(f"\nğŸ” Query: '{query}'")

    results = engine.search(
        query=query,
        collection_name="transcripts_unified",
        n_results=5
    )

    # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¨®é¡ã‚’ç¢ºèª
    source_files = set()
    for result in results['results']:
        source_file = result['metadata'].get('source_file', 'N/A')
        source_files.add(source_file)

    print(f"\nğŸ“‚ Results from {len(source_files)} source file(s):")
    for sf in sorted(source_files):
        count = sum(1 for r in results['results'] if r['metadata'].get('source_file') == sf)
        print(f"   - {sf}: {count} result(s)")

    # Top 3çµæœã®è©³ç´°è¡¨ç¤º
    print(f"\nğŸ“ Top 3 Results:")
    for i, result in enumerate(results['results'][:3], 1):
        meta = result['metadata']
        print(f"\n   [{i}] Source: {meta.get('source_file', 'N/A')}")
        print(f"       Speaker: {meta.get('speaker', 'N/A')}")
        print(f"       Timestamp: {meta.get('timestamp', 'N/A')}")
        print(f"       Similarity: {result['similarity_score']:.4f}")
        text = result['text']
        if len(text) > 100:
            text = text[:100] + "..."
        print(f"       Text: {text}")

    # æˆåŠŸåˆ¤å®š: çµæœãŒå–å¾—ã§ããŸã‹
    has_results = len(results['results']) > 0

    if has_results:
        print(f"\nâœ… Test 3 PASSED: Cross-file search working")
        return True
    else:
        print("\nâŒ Test 3 FAILED: No search results")
        return False


def test_rag_qa():
    """Test 4: RAG Q&Aå‹•ä½œç¢ºèª"""
    print("\n" + "=" * 70)
    print("Test 4: RAG Q&A System Verification")
    print("=" * 70)

    rag_system = RAGQASystem(chroma_path="chroma_db")

    # ãƒ†ã‚¹ãƒˆè³ªå•
    test_question = "æ‰æœ¬ã•ã‚“ã®è·æ­´ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
    print(f"\nâ“ Question: {test_question}")

    try:
        result = rag_system.ask(
            query=test_question,
            collection_name="transcripts_unified",
            n_contexts=3
        )

        print(f"\nğŸ’¡ Answer:")
        print(f"   {result['answer'][:200]}...")

        print(f"\nğŸ“š Contexts used: {result['num_contexts_used']}")

        # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¨®é¡ã‚’ç¢ºèª
        source_files = set()
        for ctx in result['contexts']:
            source_file = ctx['metadata'].get('source_file', 'N/A')
            source_files.add(source_file)

        print(f"   Source files: {len(source_files)}")
        for sf in sorted(source_files):
            print(f"      - {sf}")

        print(f"\nâœ… Test 4 PASSED: RAG Q&A system working")
        return True

    except Exception as e:
        print(f"\nâŒ Test 4 FAILED: {e}")
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 70)
    print("Phase 8-4: Comprehensive Verification Test")
    print("=" * 70)

    tests_passed = []

    # Test 1: è©±è€…æ¨è«–ç²¾åº¦
    tests_passed.append(("Speaker Inference", test_speaker_inference()))

    # Test 2: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£çµ±ä¸€ç‡
    tests_passed.append(("Entity Unification", test_entity_unification()))

    # Test 3: æ¨ªæ–­æ¤œç´¢
    tests_passed.append(("Cross-File Search", test_cross_file_search()))

    # Test 4: RAG Q&A
    tests_passed.append(("RAG Q&A System", test_rag_qa()))

    # æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 70)
    print("Final Test Summary")
    print("=" * 70)

    total_tests = len(tests_passed)
    passed_count = sum(1 for _, passed in tests_passed if passed)

    for test_name, passed in tests_passed:
        status = "âœ… PASSED" if passed else "âŒ FAILED"
        print(f"{status}: {test_name}")

    print(f"\nğŸ¯ Final Score: {passed_count}/{total_tests} tests passed")

    if passed_count == total_tests:
        print("\nğŸ‰ All tests passed! Phase 8 verification complete.")
        return True
    else:
        print(f"\nâš ï¸  {total_tests - passed_count} test(s) failed. Please review.")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
